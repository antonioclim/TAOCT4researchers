# Further reading — 11UNIT

## Annotated bibliography

- Porter, M. F. (1980). An algorithm for suffix stripping. *Program*, 14(3), 130–137. https://doi.org/10.1108/eb046814
- Robertson, S. (2004). Understanding inverse document frequency: on theoretical arguments for IDF. *Journal of Documentation*, 60(5), 503–520. https://doi.org/10.1108/00220410410560582
- Robertson, S., & Zaragoza, H. (2009). The probabilistic relevance framework: BM25 and beyond. *Foundations and Trends in Information Retrieval*, 3(4), 333–389. https://doi.org/10.1561/1500000019
- Joachims, T. (1998). Text categorization with support vector machines: learning with many relevant features. *European Conference on Machine Learning*, 137–142. https://doi.org/10.1007/BFb0026683
- van Rijsbergen, C. J. (1979). *Information Retrieval* (2nd ed.). (Classic monograph; no DOI.)

Each reference motivates a concrete engineering choice is treated here as a research-oriented craft
rather than a collection of library calls. The central question is how an analyst moves from raw
character sequences to defensible claims, given that token boundaries, normalisation choices and
annotation schemes impose inductive bias. We therefore separate specification from implementation:
we first state what a pipeline must preserve or discard, then encode those constraints as pure
functions with explicit preconditions. Attention is paid to failure modes, including ambiguous
segmentation, encoding artefacts and domain drift, since these frequently dominate error budgets in
empirical studies. In practice, the unit couples formal definitions with executable checks so that
each step has a measurable contract: input alphabet, transformation invariants and complexity
expectations.

Each reference motivates a concrete engineering choice is treated here as a research-oriented craft
rather than a collection of library calls. The central question is how an analyst moves from raw
character sequences to defensible claims, given that token boundaries, normalisation choices and
annotation schemes impose inductive bias. We therefore separate specification from implementation:
we first state what a pipeline must preserve or discard, then encode those constraints as pure
functions with explicit preconditions. Attention is paid to failure modes, including ambiguous
segmentation, encoding artefacts and domain drift, since these frequently dominate error budgets in
empirical studies. In practice, the unit couples formal definitions with executable checks so that
each step has a measurable contract: input alphabet, transformation invariants and complexity
expectations.

Each reference motivates a concrete engineering choice is treated here as a research-oriented craft
rather than a collection of library calls. The central question is how an analyst moves from raw
character sequences to defensible claims, given that token boundaries, normalisation choices and
annotation schemes impose inductive bias. We therefore separate specification from implementation:
we first state what a pipeline must preserve or discard, then encode those constraints as pure
functions with explicit preconditions. Attention is paid to failure modes, including ambiguous
segmentation, encoding artefacts and domain drift, since these frequently dominate error budgets in
empirical studies. In practice, the unit couples formal definitions with executable checks so that
each step has a measurable contract: input alphabet, transformation invariants and complexity
expectations.

Each reference motivates a concrete engineering choice is treated here as a research-oriented craft
rather than a collection of library calls. The central question is how an analyst moves from raw
character sequences to defensible claims, given that token boundaries, normalisation choices and
annotation schemes impose inductive bias. We therefore separate specification from implementation:
we first state what a pipeline must preserve or discard, then encode those constraints as pure
functions with explicit preconditions. Attention is paid to failure modes, including ambiguous
segmentation, encoding artefacts and domain drift, since these frequently dominate error budgets in
empirical studies. In practice, the unit couples formal definitions with executable checks so that
each step has a measurable contract: input alphabet, transformation invariants and complexity
expectations.

Each reference motivates a concrete engineering choice is treated here as a research-oriented craft
rather than a collection of library calls. The central question is how an analyst moves from raw
character sequences to defensible claims, given that token boundaries, normalisation choices and
annotation schemes impose inductive bias. We therefore separate specification from implementation:
we first state what a pipeline must preserve or discard, then encode those constraints as pure
functions with explicit preconditions. Attention is paid to failure modes, including ambiguous
segmentation, encoding artefacts and domain drift, since these frequently dominate error budgets in
empirical studies. In practice, the unit couples formal definitions with executable checks so that
each step has a measurable contract: input alphabet, transformation invariants and complexity
expectations.
