# Homework — 11UNIT

## Instructions

The homework links modelling choices to empirical consequences is treated here as a research-
oriented craft rather than a collection of library calls. The central question is how an analyst
moves from raw character sequences to defensible claims, given that token boundaries, normalisation
choices and annotation schemes impose inductive bias. We therefore separate specification from
implementation: we first state what a pipeline must preserve or discard, then encode those
constraints as pure functions with explicit preconditions. Attention is paid to failure modes,
including ambiguous segmentation, encoding artefacts and domain drift, since these frequently
dominate error budgets in empirical studies. In practice, the unit couples formal definitions with
executable checks so that each step has a measurable contract: input alphabet, transformation
invariants and complexity expectations.

The homework links modelling choices to empirical consequences is treated here as a research-
oriented craft rather than a collection of library calls. The central question is how an analyst
moves from raw character sequences to defensible claims, given that token boundaries, normalisation
choices and annotation schemes impose inductive bias. We therefore separate specification from
implementation: we first state what a pipeline must preserve or discard, then encode those
constraints as pure functions with explicit preconditions. Attention is paid to failure modes,
including ambiguous segmentation, encoding artefacts and domain drift, since these frequently
dominate error budgets in empirical studies. In practice, the unit couples formal definitions with
executable checks so that each step has a measurable contract: input alphabet, transformation
invariants and complexity expectations.

The homework links modelling choices to empirical consequences is treated here as a research-
oriented craft rather than a collection of library calls. The central question is how an analyst
moves from raw character sequences to defensible claims, given that token boundaries, normalisation
choices and annotation schemes impose inductive bias. We therefore separate specification from
implementation: we first state what a pipeline must preserve or discard, then encode those
constraints as pure functions with explicit preconditions. Attention is paid to failure modes,
including ambiguous segmentation, encoding artefacts and domain drift, since these frequently
dominate error budgets in empirical studies. In practice, the unit couples formal definitions with
executable checks so that each step has a measurable contract: input alphabet, transformation
invariants and complexity expectations.

The homework links modelling choices to empirical consequences is treated here as a research-
oriented craft rather than a collection of library calls. The central question is how an analyst
moves from raw character sequences to defensible claims, given that token boundaries, normalisation
choices and annotation schemes impose inductive bias. We therefore separate specification from
implementation: we first state what a pipeline must preserve or discard, then encode those
constraints as pure functions with explicit preconditions. Attention is paid to failure modes,
including ambiguous segmentation, encoding artefacts and domain drift, since these frequently
dominate error budgets in empirical studies. In practice, the unit couples formal definitions with
executable checks so that each step has a measurable contract: input alphabet, transformation
invariants and complexity expectations.

The homework links modelling choices to empirical consequences is treated here as a research-
oriented craft rather than a collection of library calls. The central question is how an analyst
moves from raw character sequences to defensible claims, given that token boundaries, normalisation
choices and annotation schemes impose inductive bias. We therefore separate specification from
implementation: we first state what a pipeline must preserve or discard, then encode those
constraints as pure functions with explicit preconditions. Attention is paid to failure modes,
including ambiguous segmentation, encoding artefacts and domain drift, since these frequently
dominate error budgets in empirical studies. In practice, the unit couples formal definitions with
executable checks so that each step has a measurable contract: input alphabet, transformation
invariants and complexity expectations.

The homework links modelling choices to empirical consequences is treated here as a research-
oriented craft rather than a collection of library calls. The central question is how an analyst
moves from raw character sequences to defensible claims, given that token boundaries, normalisation
choices and annotation schemes impose inductive bias. We therefore separate specification from
implementation: we first state what a pipeline must preserve or discard, then encode those
constraints as pure functions with explicit preconditions. Attention is paid to failure modes,
including ambiguous segmentation, encoding artefacts and domain drift, since these frequently
dominate error budgets in empirical studies. In practice, the unit couples formal definitions with
executable checks so that each step has a measurable contract: input alphabet, transformation
invariants and complexity expectations.

The homework links modelling choices to empirical consequences is treated here as a research-
oriented craft rather than a collection of library calls. The central question is how an analyst
moves from raw character sequences to defensible claims, given that token boundaries, normalisation
choices and annotation schemes impose inductive bias. We therefore separate specification from
implementation: we first state what a pipeline must preserve or discard, then encode those
constraints as pure functions with explicit preconditions. Attention is paid to failure modes,
including ambiguous segmentation, encoding artefacts and domain drift, since these frequently
dominate error budgets in empirical studies. In practice, the unit couples formal definitions with
executable checks so that each step has a measurable contract: input alphabet, transformation
invariants and complexity expectations.

The homework links modelling choices to empirical consequences is treated here as a research-
oriented craft rather than a collection of library calls. The central question is how an analyst
moves from raw character sequences to defensible claims, given that token boundaries, normalisation
choices and annotation schemes impose inductive bias. We therefore separate specification from
implementation: we first state what a pipeline must preserve or discard, then encode those
constraints as pure functions with explicit preconditions. Attention is paid to failure modes,
including ambiguous segmentation, encoding artefacts and domain drift, since these frequently
dominate error budgets in empirical studies. In practice, the unit couples formal definitions with
executable checks so that each step has a measurable contract: input alphabet, transformation
invariants and complexity expectations.

## Tasks

1. **Encoding audit**: Collect three short text files from distinct sources (e.g., HTML, PDF-extracted and plain text). Identify encoding issues and report how you detected and corrected them.
2. **Regex extraction**: Design a small schema for a semi-structured log format and write a parser that extracts records into a CSV table. Provide tests and report extraction accuracy.
3. **Tokenisation study**: Implement two tokenisers: a conservative word tokeniser and a character n-gram tokeniser. Compare vocabulary growth curves on the same corpus.
4. **TF–IDF classification**: Build a binary classifier over a labelled subset of your corpus. Report $F_1$, confusion matrix and an error analysis of at least ten misclassified documents.
5. **Reflection**: Discuss which parts of the pipeline are assumptions about the data and which parts are conclusions from the data.

Submission is treated here as a research-oriented craft rather than a collection of library calls.
The central question is how an analyst moves from raw character sequences to defensible claims,
given that token boundaries, normalisation choices and annotation schemes impose inductive bias. We
therefore separate specification from implementation: we first state what a pipeline must preserve
or discard, then encode those constraints as pure functions with explicit preconditions. Attention
is paid to failure modes, including ambiguous segmentation, encoding artefacts and domain drift,
since these frequently dominate error budgets in empirical studies. In practice, the unit couples
formal definitions with executable checks so that each step has a measurable contract: input
alphabet, transformation invariants and complexity expectations.

Submission is treated here as a research-oriented craft rather than a collection of library calls.
The central question is how an analyst moves from raw character sequences to defensible claims,
given that token boundaries, normalisation choices and annotation schemes impose inductive bias. We
therefore separate specification from implementation: we first state what a pipeline must preserve
or discard, then encode those constraints as pure functions with explicit preconditions. Attention
is paid to failure modes, including ambiguous segmentation, encoding artefacts and domain drift,
since these frequently dominate error budgets in empirical studies. In practice, the unit couples
formal definitions with executable checks so that each step has a measurable contract: input
alphabet, transformation invariants and complexity expectations.

Submission is treated here as a research-oriented craft rather than a collection of library calls.
The central question is how an analyst moves from raw character sequences to defensible claims,
given that token boundaries, normalisation choices and annotation schemes impose inductive bias. We
therefore separate specification from implementation: we first state what a pipeline must preserve
or discard, then encode those constraints as pure functions with explicit preconditions. Attention
is paid to failure modes, including ambiguous segmentation, encoding artefacts and domain drift,
since these frequently dominate error budgets in empirical studies. In practice, the unit couples
formal definitions with executable checks so that each step has a measurable contract: input
alphabet, transformation invariants and complexity expectations.

Submission is treated here as a research-oriented craft rather than a collection of library calls.
The central question is how an analyst moves from raw character sequences to defensible claims,
given that token boundaries, normalisation choices and annotation schemes impose inductive bias. We
therefore separate specification from implementation: we first state what a pipeline must preserve
or discard, then encode those constraints as pure functions with explicit preconditions. Attention
is paid to failure modes, including ambiguous segmentation, encoding artefacts and domain drift,
since these frequently dominate error budgets in empirical studies. In practice, the unit couples
formal definitions with executable checks so that each step has a measurable contract: input
alphabet, transformation invariants and complexity expectations.

Submission is treated here as a research-oriented craft rather than a collection of library calls.
The central question is how an analyst moves from raw character sequences to defensible claims,
given that token boundaries, normalisation choices and annotation schemes impose inductive bias. We
therefore separate specification from implementation: we first state what a pipeline must preserve
or discard, then encode those constraints as pure functions with explicit preconditions. Attention
is paid to failure modes, including ambiguous segmentation, encoding artefacts and domain drift,
since these frequently dominate error budgets in empirical studies. In practice, the unit couples
formal definitions with executable checks so that each step has a measurable contract: input
alphabet, transformation invariants and complexity expectations.

Submission is treated here as a research-oriented craft rather than a collection of library calls.
The central question is how an analyst moves from raw character sequences to defensible claims,
given that token boundaries, normalisation choices and annotation schemes impose inductive bias. We
therefore separate specification from implementation: we first state what a pipeline must preserve
or discard, then encode those constraints as pure functions with explicit preconditions. Attention
is paid to failure modes, including ambiguous segmentation, encoding artefacts and domain drift,
since these frequently dominate error budgets in empirical studies. In practice, the unit couples
formal definitions with executable checks so that each step has a measurable contract: input
alphabet, transformation invariants and complexity expectations.

Submission is treated here as a research-oriented craft rather than a collection of library calls.
The central question is how an analyst moves from raw character sequences to defensible claims,
given that token boundaries, normalisation choices and annotation schemes impose inductive bias. We
therefore separate specification from implementation: we first state what a pipeline must preserve
or discard, then encode those constraints as pure functions with explicit preconditions. Attention
is paid to failure modes, including ambiguous segmentation, encoding artefacts and domain drift,
since these frequently dominate error budgets in empirical studies. In practice, the unit couples
formal definitions with executable checks so that each step has a measurable contract: input
alphabet, transformation invariants and complexity expectations.

## Reporting requirements

Reports are expected to include reproducible commands and an appendix of error cases is treated here
as a research-oriented craft rather than a collection of library calls. The central question is how
an analyst moves from raw character sequences to defensible claims, given that token boundaries,
normalisation choices and annotation schemes impose inductive bias. We therefore separate
specification from implementation: we first state what a pipeline must preserve or discard, then
encode those constraints as pure functions with explicit preconditions. Attention is paid to failure
modes, including ambiguous segmentation, encoding artefacts and domain drift, since these frequently
dominate error budgets in empirical studies. In practice, the unit couples formal definitions with
executable checks so that each step has a measurable contract: input alphabet, transformation
invariants and complexity expectations.

Reports are expected to include reproducible commands and an appendix of error cases is treated here
as a research-oriented craft rather than a collection of library calls. The central question is how
an analyst moves from raw character sequences to defensible claims, given that token boundaries,
normalisation choices and annotation schemes impose inductive bias. We therefore separate
specification from implementation: we first state what a pipeline must preserve or discard, then
encode those constraints as pure functions with explicit preconditions. Attention is paid to failure
modes, including ambiguous segmentation, encoding artefacts and domain drift, since these frequently
dominate error budgets in empirical studies. In practice, the unit couples formal definitions with
executable checks so that each step has a measurable contract: input alphabet, transformation
invariants and complexity expectations.

Reports are expected to include reproducible commands and an appendix of error cases is treated here
as a research-oriented craft rather than a collection of library calls. The central question is how
an analyst moves from raw character sequences to defensible claims, given that token boundaries,
normalisation choices and annotation schemes impose inductive bias. We therefore separate
specification from implementation: we first state what a pipeline must preserve or discard, then
encode those constraints as pure functions with explicit preconditions. Attention is paid to failure
modes, including ambiguous segmentation, encoding artefacts and domain drift, since these frequently
dominate error budgets in empirical studies. In practice, the unit couples formal definitions with
executable checks so that each step has a measurable contract: input alphabet, transformation
invariants and complexity expectations.

Reports are expected to include reproducible commands and an appendix of error cases is treated here
as a research-oriented craft rather than a collection of library calls. The central question is how
an analyst moves from raw character sequences to defensible claims, given that token boundaries,
normalisation choices and annotation schemes impose inductive bias. We therefore separate
specification from implementation: we first state what a pipeline must preserve or discard, then
encode those constraints as pure functions with explicit preconditions. Attention is paid to failure
modes, including ambiguous segmentation, encoding artefacts and domain drift, since these frequently
dominate error budgets in empirical studies. In practice, the unit couples formal definitions with
executable checks so that each step has a measurable contract: input alphabet, transformation
invariants and complexity expectations.

Reports are expected to include reproducible commands and an appendix of error cases is treated here
as a research-oriented craft rather than a collection of library calls. The central question is how
an analyst moves from raw character sequences to defensible claims, given that token boundaries,
normalisation choices and annotation schemes impose inductive bias. We therefore separate
specification from implementation: we first state what a pipeline must preserve or discard, then
encode those constraints as pure functions with explicit preconditions. Attention is paid to failure
modes, including ambiguous segmentation, encoding artefacts and domain drift, since these frequently
dominate error budgets in empirical studies. In practice, the unit couples formal definitions with
executable checks so that each step has a measurable contract: input alphabet, transformation
invariants and complexity expectations.
