<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>11UNIT: Text Processing and NLP Fundamentals | Computational Thinking</title>
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/reveal.js@5.0.4/dist/reset.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/reveal.js@5.0.4/dist/reveal.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/reveal.js@5.0.4/dist/theme/night.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/reveal.js@5.0.4/plugin/highlight/monokai.css">
    
    <style>
        /* === BASE STYLES === */
        :root {
            --r-heading-color: #58a6ff;
            --primary-blue: #4a9eff;
            --success-green: #3fb950;
            --warning-red: #f85149;
            --warning-orange: #ffa657;
            --text-muted: #8b949e;
            --border-color: #30363d;
            --bg-dark: #0d1117;
            --bg-card: #161b22;
        }
        
        .reveal h1, .reveal h2 { text-transform: none; }
        .reveal .slides section { text-align: left; }
        
        /* === RESPONSIVE BREAKPOINTS === */
        @media (max-width: 480px) {
            .reveal h1 { font-size: 1.5em !important; }
            .reveal h2 { font-size: 1.2em !important; }
            .reveal p, .reveal li { font-size: 0.8em !important; }
            .two-columns { flex-direction: column !important; }
            .diagram-box { font-size: 0.4em !important; }
        }
        
        @media (max-width: 768px) {
            .reveal h1 { font-size: 1.8em !important; }
            .reveal h2 { font-size: 1.4em !important; }
            table { font-size: 0.6em !important; }
            .two-columns { gap: 15px !important; }
        }
        
        @media (min-width: 1024px) {
            .reveal .slides section {
                padding: 20px 40px;
            }
        }
        
        /* === COMPONENT STYLES === */
        .hook-box {
            background: linear-gradient(135deg, #1a1a2e 0%, #16213e 100%);
            border-left: 4px solid #e94560;
            padding: 20px 25px;
            margin: 20px 0;
            border-radius: 0 8px 8px 0;
            font-style: italic;
        }
        .hook-box .source {
            font-size: 0.7em;
            color: var(--text-muted);
            margin-top: 10px;
            font-style: normal;
        }
        
        .concept-box {
            background: linear-gradient(135deg, #1a2d1a 0%, #16213e 100%);
            border-left: 4px solid var(--success-green);
            padding: 15px 20px;
            margin: 15px 0;
            border-radius: 0 8px 8px 0;
        }
        .concept-box h4 {
            color: var(--success-green);
            margin: 0 0 10px 0;
        }
        
        .warning-box {
            background: linear-gradient(135deg, #2d1b1b 0%, #16213e 100%);
            border: 1px solid var(--warning-red);
            padding: 20px;
            margin: 15px 0;
            border-radius: 8px;
        }
        .warning-box h4 {
            color: var(--warning-red);
            margin: 0 0 10px 0;
        }
        
        .tip-box {
            background: linear-gradient(135deg, #2d2a1b 0%, #16213e 100%);
            border: 1px solid var(--warning-orange);
            padding: 15px 20px;
            margin: 15px 0;
            border-radius: 8px;
        }
        .tip-box h4 {
            color: var(--warning-orange);
            margin: 0 0 10px 0;
        }
        
        .diagram-box {
            background: var(--bg-dark);
            border: 1px solid var(--border-color);
            padding: 20px;
            border-radius: 8px;
            font-family: 'Courier New', monospace;
            font-size: 0.55em;
            line-height: 1.3;
            overflow-x: auto;
        }
        
        .two-columns {
            display: flex;
            gap: 30px;
        }
        .two-columns > div { flex: 1; }
        
        .three-columns {
            display: flex;
            gap: 20px;
        }
        .three-columns > div { flex: 1; }
        
        .highlight-text {
            color: #ffa657;
            font-weight: bold;
        }
        .code-highlight {
            color: #79c0ff;
            font-family: 'Courier New', monospace;
        }
        .small-text {
            font-size: 0.75em;
            color: var(--text-muted);
        }
        
        table {
            width: 100%;
            border-collapse: collapse;
            margin: 15px 0;
            font-size: 0.7em;
        }
        th, td {
            border: 1px solid var(--border-color);
            padding: 8px;
            text-align: left;
        }
        th {
            background: var(--bg-card);
            color: var(--primary-blue);
        }
        
        .regex-pattern {
            background: var(--bg-card);
            border: 1px solid var(--primary-blue);
            padding: 8px 15px;
            border-radius: 4px;
            font-family: 'Courier New', monospace;
            font-size: 1.1em;
            color: #f0883e;
        }
        
        .metachar-grid {
            display: grid;
            grid-template-columns: repeat(4, 1fr);
            gap: 10px;
            font-family: 'Courier New', monospace;
        }
        .metachar-item {
            background: var(--bg-card);
            padding: 8px;
            border-radius: 4px;
            text-align: center;
        }
        .metachar-item span {
            color: var(--warning-orange);
            font-size: 1.2em;
        }
        
        .pipeline-arrow {
            color: var(--primary-blue);
            font-size: 1.5em;
            margin: 0 10px;
        }
        
        .learning-obj {
            background: var(--bg-card);
            border-radius: 8px;
            padding: 15px;
            margin: 10px 0;
        }
        .learning-obj .level {
            color: var(--warning-orange);
            font-size: 0.8em;
        }
    </style>
</head>
<body>
    <div class="reveal">
        <div class="slides">
            
            <!-- ==================== SLIDE 1: TITLE ==================== -->
            <section data-background-gradient="linear-gradient(135deg, #0d1117 0%, #161b22 100%)">
                <h1 style="font-size: 2em;">ğŸ“ Unit 11</h1>
                <h2 style="color: #58a6ff; font-size: 1.5em;">TEXT PROCESSING AND NLP FUNDAMENTALS</h2>
                <p style="color: #8b949e; font-size: 0.9em;">
                    Regular Expressions â€¢ String Operations â€¢ Tokenisation â€¢ Text Features
                </p>
                <hr style="border-color: #30363d; margin: 30px 0;">
                <p style="font-size: 0.7em; color: #8b949e;">
                    THE ART OF COMPUTATIONAL THINKING FOR RESEARCHERS<br>
                    Version 4.0 â€” January 2025
                </p>
                <aside class="notes">
                    Welcome to Unit 11. Today we explore the computational treatment of textual dataâ€”from pattern matching to NLP fundamentals.
                </aside>
            </section>
            
            <!-- ==================== SLIDE 2: HOOK ==================== -->
            <section>
                <h2>ğŸ“š The Computational Turn in Text Analysis</h2>
                
                <div class="hook-box">
                    <p>
                        "In literary studies, computational text analysis has uncovered 
                        <strong>authorship patterns and thematic developments</strong> 
                        across centuries of literature."
                    </p>
                    <p class="source">
                        â€” Jockers & Underwood, "Text-mining the humanities" (2016, p. 292)
                    </p>
                </div>
                
                <p class="fragment">
                    Text processing transforms <span class="highlight-text">unstructured character sequences</span> 
                    into <span class="highlight-text">structured data</span> amenable to computational analysis.
                </p>
                
                <p class="fragment small-text">
                    This session: from raw text to research-ready features.
                </p>
                <aside class="notes">
                    The computational analysis of text has transformed multiple disciplines. Today we learn the foundational techniques.
                </aside>
            </section>
            
            <!-- ==================== SLIDE 3: AGENDA ==================== -->
            <section>
                <h2>ğŸ“‹ Session Structure</h2>
                
                <table>
                    <tr><th>Time</th><th>Activity</th></tr>
                    <tr><td>0:00â€“0:45</td><td>Theory: Regular Expressions</td></tr>
                    <tr><td>0:45â€“1:30</td><td>Lab: Regex and String Operations</td></tr>
                    <tr><td>1:30â€“1:45</td><td>â˜• Break</td></tr>
                    <tr><td>1:45â€“2:30</td><td>Theory: NLP Fundamentals</td></tr>
                    <tr><td>2:30â€“3:30</td><td>Lab: Tokenisation, Normalisation, TF-IDF</td></tr>
                    <tr><td>3:30â€“4:00</td><td>Pipeline Design Workshop</td></tr>
                </table>
                
                <p class="small-text" style="margin-top: 20px;">
                    <strong>Prerequisites:</strong> 10UNIT (File I/O), 04UNIT (Data Structures)<br>
                    <strong>Prepares for:</strong> 13UNIT (Machine Learning)
                </p>
                <aside class="notes">
                    We balance theory with hands-on practice. The skills build toward ML text features.
                </aside>
            </section>
            
            <!-- ==================== SLIDE 4: LEARNING OBJECTIVES ==================== -->
            <section>
                <h2>ğŸ¯ Learning Objectives</h2>
                
                <div class="learning-obj">
                    <span class="level">UNDERSTAND</span>
                    <p>LO1: Explain regex syntax, metacharacters and matching semantics</p>
                </div>
                
                <div class="learning-obj">
                    <span class="level">APPLY</span>
                    <p>LO2: Implement text extraction and validation using regex patterns</p>
                    <p>LO3: Build text preprocessing pipelines</p>
                    <p>LO4: Apply NLP techniques (stemming, lemmatisation, POS, n-grams)</p>
                </div>
                
                <div class="learning-obj">
                    <span class="level">ANALYSE</span>
                    <p>LO5: Analyse text corpora using frequency analysis and TF-IDF</p>
                </div>
                
                <div class="learning-obj">
                    <span class="level">CREATE</span>
                    <p>LO6: Design end-to-end text processing pipelines</p>
                </div>
                <aside class="notes">
                    Six objectives spanning four cognitive levels. We progress from understanding to creation.
                </aside>
            </section>
            
            <!-- ==================== PART I: REGULAR EXPRESSIONS ==================== -->
            <section>
                <section>
                    <h1>PART I</h1>
                    <h2 style="color: #58a6ff;">Regular Expressions</h2>
                    <p class="small-text">The language of patterns</p>
                </section>
                
                <!-- SLIDE 5: REGEX INTRODUCTION -->
                <section>
                    <h2>What Are Regular Expressions?</h2>
                    
                    <div class="concept-box">
                        <h4>ğŸ’¡ Definition</h4>
                        <p>Regular expressions (regex) are a <strong>formal language</strong> for specifying text patterns. They describe character sequences using metacharacters and operators.</p>
                    </div>
                    
                    <div class="two-columns fragment">
                        <div>
                            <h4>Without Regex</h4>
                            <pre><code class="language-python">def find_emails(text):
    emails = []
    # 50+ lines of loops,
    # conditionals, state
    # tracking...
    return emails</code></pre>
                        </div>
                        <div>
                            <h4>With Regex</h4>
                            <pre><code class="language-python">import re

pattern = r'\b[\w.]+@[\w.]+\.\w+\b'
emails = re.findall(pattern, text)</code></pre>
                        </div>
                    </div>
                    <aside class="notes">
                        Regex provides concise pattern specification. One line replaces dozens.
                    </aside>
                </section>
                
                <!-- SLIDE 6: METACHARACTERS -->
                <section>
                    <h2>Metacharacters: The Building Blocks</h2>
                    
                    <p>These characters have special meaning in regex:</p>
                    
                    <div class="metachar-grid">
                        <div class="metachar-item"><span>.</span><br>any char</div>
                        <div class="metachar-item"><span>^</span><br>start</div>
                        <div class="metachar-item"><span>$</span><br>end</div>
                        <div class="metachar-item"><span>*</span><br>0+ times</div>
                        <div class="metachar-item"><span>+</span><br>1+ times</div>
                        <div class="metachar-item"><span>?</span><br>0 or 1</div>
                        <div class="metachar-item"><span>{ }</span><br>count</div>
                        <div class="metachar-item"><span>[ ]</span><br>char class</div>
                        <div class="metachar-item"><span>\</span><br>escape</div>
                        <div class="metachar-item"><span>|</span><br>alternation</div>
                        <div class="metachar-item"><span>( )</span><br>group</div>
                        <div class="metachar-item"><span>\b</span><br>boundary</div>
                    </div>
                    
                    <p class="small-text fragment" style="margin-top: 20px;">
                        To match a literal metacharacter, escape it: <code class="code-highlight">\.</code> matches a period
                    </p>
                    <aside class="notes">
                        Twelve core metacharacters. Each serves a distinct purpose in pattern specification.
                    </aside>
                </section>
                
                <!-- SLIDE 7: CHARACTER CLASSES -->
                <section>
                    <h2>Character Classes</h2>
                    
                    <table>
                        <tr><th>Pattern</th><th>Matches</th><th>Example</th></tr>
                        <tr><td><code>[abc]</code></td><td>a, b, or c</td><td><code>[aeiou]</code> â†’ vowels</td></tr>
                        <tr><td><code>[a-z]</code></td><td>lowercase letters</td><td><code>[A-Za-z]</code> â†’ all letters</td></tr>
                        <tr><td><code>[^abc]</code></td><td>NOT a, b, or c</td><td><code>[^0-9]</code> â†’ non-digits</td></tr>
                        <tr><td><code>\d</code></td><td>digit [0-9]</td><td><code>\d{4}</code> â†’ 4 digits</td></tr>
                        <tr><td><code>\w</code></td><td>word char [a-zA-Z0-9_]</td><td><code>\w+</code> â†’ word</td></tr>
                        <tr><td><code>\s</code></td><td>whitespace</td><td><code>\s+</code> â†’ spaces</td></tr>
                    </table>
                    
                    <div class="tip-box fragment">
                        <h4>ğŸ’¡ Uppercase = Negation</h4>
                        <p><code>\D</code> = non-digit, <code>\W</code> = non-word, <code>\S</code> = non-whitespace</p>
                    </div>
                    <aside class="notes">
                        Character classes match sets of characters. Predefined classes simplify common patterns.
                    </aside>
                </section>
                
                <!-- SLIDE 8: QUANTIFIERS -->
                <section>
                    <h2>Quantifiers: Controlling Repetition</h2>
                    
                    <table>
                        <tr><th>Quantifier</th><th>Meaning</th><th>Example</th></tr>
                        <tr><td><code>*</code></td><td>zero or more</td><td><code>ab*c</code> â†’ ac, abc, abbc...</td></tr>
                        <tr><td><code>+</code></td><td>one or more</td><td><code>ab+c</code> â†’ abc, abbc...</td></tr>
                        <tr><td><code>?</code></td><td>zero or one</td><td><code>colou?r</code> â†’ color, colour</td></tr>
                        <tr><td><code>{n}</code></td><td>exactly n</td><td><code>\d{4}</code> â†’ 2025</td></tr>
                        <tr><td><code>{n,m}</code></td><td>n to m times</td><td><code>\d{2,4}</code> â†’ 25, 025, 2025</td></tr>
                        <tr><td><code>{n,}</code></td><td>n or more</td><td><code>\w{3,}</code> â†’ 3+ chars</td></tr>
                    </table>
                    
                    <div class="warning-box fragment">
                        <h4>âš ï¸ Greedy vs Non-Greedy</h4>
                        <p>Default quantifiers are <strong>greedy</strong> (match maximally).<br>
                        Append <code>?</code> for non-greedy: <code>.*?</code> matches minimally.</p>
                    </div>
                    <aside class="notes">
                        Quantifiers control how many times a pattern element matches. Greedy vs non-greedy is a common source of bugs.
                    </aside>
                </section>
                
                <!-- SLIDE 9: ANCHORS -->
                <section>
                    <h2>Anchors: Position Without Consuming</h2>
                    
                    <div class="diagram-box">
<pre>
    Text: "Hello World"
    
    ^Hello       âœ“ matches (Hello at START)
    World$       âœ“ matches (World at END)
    ^World       âœ— no match (World not at start)
    
    Text: "cat category cathedral"
    
    \bcat\b      âœ“ matches "cat" only (word boundary)
    cat          âœ“ matches ALL three occurrences
</pre>
                    </div>
                    
                    <table style="margin-top: 15px;">
                        <tr><th>Anchor</th><th>Position</th></tr>
                        <tr><td><code>^</code></td><td>Start of string/line</td></tr>
                        <tr><td><code>$</code></td><td>End of string/line</td></tr>
                        <tr><td><code>\b</code></td><td>Word boundary</td></tr>
                        <tr><td><code>\B</code></td><td>Non-boundary</td></tr>
                    </table>
                    <aside class="notes">
                        Anchors assert position without consuming characters. Essential for precise matching.
                    </aside>
                </section>
                
                <!-- SLIDE 10: GROUPS -->
                <section>
                    <h2>Groups: Capturing and Structure</h2>
                    
                    <pre><code class="language-python">import re

text = "Contact: john@example.com"

# Capturing groups with ()
pattern = r'(\w+)@(\w+)\.(\w+)'
match = re.search(pattern, text)

print(match.group(0))  # john@example.com (full match)
print(match.group(1))  # john (first group)
print(match.group(2))  # example (second group)
print(match.group(3))  # com (third group)</code></pre>
                    
                    <div class="two-columns fragment">
                        <div>
                            <h4>Non-capturing</h4>
                            <code class="regex-pattern">(?:pattern)</code>
                            <p class="small-text">Groups without capturing</p>
                        </div>
                        <div>
                            <h4>Named groups</h4>
                            <code class="regex-pattern">(?P&lt;name&gt;pattern)</code>
                            <p class="small-text">Access by name</p>
                        </div>
                    </div>
                    <aside class="notes">
                        Groups enable extraction and provide structure. Named groups improve code readability.
                    </aside>
                </section>
                
                <!-- SLIDE 11: NAMED GROUPS EXAMPLE -->
                <section>
                    <h2>Named Groups in Practice</h2>
                    
                    <pre><code class="language-python">import re

log = "2025-01-17 14:30:25 ERROR Connection failed"

pattern = r'''
    (?P&lt;date&gt;\d{4}-\d{2}-\d{2})\s+
    (?P&lt;time&gt;\d{2}:\d{2}:\d{2})\s+
    (?P&lt;level&gt;\w+)\s+
    (?P&lt;message&gt;.+)
'''

match = re.search(pattern, log, re.VERBOSE)

if match:
    print(match.group('date'))    # 2025-01-17
    print(match.group('level'))   # ERROR
    print(match.group('message')) # Connection failed
    
    # Or as dictionary
    print(match.groupdict())
    # {'date': '2025-01-17', 'time': '14:30:25', ...}</code></pre>
                    <aside class="notes">
                        Named groups make extraction code self-documenting. The VERBOSE flag permits whitespace and comments.
                    </aside>
                </section>
                
                <!-- SLIDE 12: LOOKAHEAD/LOOKBEHIND -->
                <section>
                    <h2>Lookahead and Lookbehind</h2>
                    
                    <p>Zero-width assertions that examine context without consuming:</p>
                    
                    <table>
                        <tr><th>Assertion</th><th>Syntax</th><th>Meaning</th></tr>
                        <tr><td>Positive lookahead</td><td><code>(?=...)</code></td><td>Followed by...</td></tr>
                        <tr><td>Negative lookahead</td><td><code>(?!...)</code></td><td>NOT followed by...</td></tr>
                        <tr><td>Positive lookbehind</td><td><code>(?&lt;=...)</code></td><td>Preceded by...</td></tr>
                        <tr><td>Negative lookbehind</td><td><code>(?&lt;!...)</code></td><td>NOT preceded by...</td></tr>
                    </table>
                    
                    <pre class="fragment"><code class="language-python"># Match numbers preceded by $
pattern = r'(?<=\$)\d+'
re.findall(pattern, "Price: $100, â‚¬50, $200")  # ['100', '200']

# Match word NOT followed by 'ing'
pattern = r'\b\w+(?!ing)\b'</code></pre>
                    <aside class="notes">
                        Lookahead and lookbehind enable context-sensitive matching without including context in the match.
                    </aside>
                </section>
                
                <!-- SLIDE 13: RE MODULE -->
                <section>
                    <h2>The Python re Module</h2>
                    
                    <table>
                        <tr><th>Function</th><th>Returns</th><th>Use Case</th></tr>
                        <tr><td><code>re.search()</code></td><td>Match or None</td><td>Find first occurrence</td></tr>
                        <tr><td><code>re.match()</code></td><td>Match or None</td><td>Match at start only</td></tr>
                        <tr><td><code>re.findall()</code></td><td>List of strings</td><td>All non-overlapping matches</td></tr>
                        <tr><td><code>re.finditer()</code></td><td>Iterator of Match</td><td>Iterate with positions</td></tr>
                        <tr><td><code>re.sub()</code></td><td>Modified string</td><td>Search and replace</td></tr>
                        <tr><td><code>re.split()</code></td><td>List of strings</td><td>Split on pattern</td></tr>
                        <tr><td><code>re.compile()</code></td><td>Pattern object</td><td>Reusable pattern</td></tr>
                    </table>
                    
                    <div class="tip-box fragment">
                        <h4>ğŸ’¡ Performance Tip</h4>
                        <p>Compile patterns used multiple times: <code>pattern = re.compile(r'...')</code></p>
                    </div>
                    <aside class="notes">
                        Select the appropriate function for your use case. Compile for repeated use.
                    </aside>
                </section>
                
                <!-- SLIDE 14: FLAGS -->
                <section>
                    <h2>Regex Flags</h2>
                    
                    <table>
                        <tr><th>Flag</th><th>Short</th><th>Effect</th></tr>
                        <tr><td><code>re.IGNORECASE</code></td><td><code>re.I</code></td><td>Case-insensitive matching</td></tr>
                        <tr><td><code>re.MULTILINE</code></td><td><code>re.M</code></td><td>^ and $ match line boundaries</td></tr>
                        <tr><td><code>re.DOTALL</code></td><td><code>re.S</code></td><td>. matches newlines</td></tr>
                        <tr><td><code>re.VERBOSE</code></td><td><code>re.X</code></td><td>Allow whitespace/comments</td></tr>
                    </table>
                    
                    <pre class="fragment"><code class="language-python"># Combine flags with |
pattern = re.compile(
    r'''
        \b              # word boundary
        (?P&lt;word&gt;\w+)   # capture the word
        \b              # word boundary
    ''',
    re.IGNORECASE | re.VERBOSE
)</code></pre>
                    <aside class="notes">
                        Flags modify pattern behaviour. VERBOSE is invaluable for complex patterns.
                    </aside>
                </section>
                
                <!-- SLIDE 15: PRACTICAL PATTERNS -->
                <section>
                    <h2>Practical Pattern Examples</h2>
                    
                    <pre><code class="language-python"># Email (simplified)
email = r'\b[\w.+-]+@[\w.-]+\.\w{2,}\b'

# URL
url = r'https?://[\w.-]+(?:/[\w./-]*)?'

# Date (ISO format)
date_iso = r'\d{4}-(?:0[1-9]|1[0-2])-(?:0[1-9]|[12]\d|3[01])'

# Phone (various formats)
phone = r'(?:\+\d{1,3}[-.\s]?)?\(?\d{3}\)?[-.\s]?\d{3}[-.\s]?\d{4}'

# HTML tag
html_tag = r'<(?P&lt;tag&gt;\w+)[^>]*>(?P&lt;content&gt;.*?)</(?P=tag)>'

# Sentence boundary (simplified)
sentence = r'(?<=[.!?])\s+(?=[A-Z])'</code></pre>
                    <aside class="notes">
                        These patterns demonstrate common extraction tasks. Each handles edge cases.
                    </aside>
                </section>
            </section>
            
            <!-- ==================== PART II: STRING OPERATIONS ==================== -->
            <section>
                <section>
                    <h1>PART II</h1>
                    <h2 style="color: #58a6ff;">String Operations</h2>
                    <p class="small-text">Beyond regular expressions</p>
                </section>
                
                <!-- SLIDE 16: STRING METHODS -->
                <section>
                    <h2>Essential String Methods</h2>
                    
                    <div class="three-columns">
                        <div>
                            <h4>Case</h4>
                            <pre><code class="language-python">s.upper()
s.lower()
s.title()
s.capitalize()
s.swapcase()</code></pre>
                        </div>
                        <div>
                            <h4>Whitespace</h4>
                            <pre><code class="language-python">s.strip()
s.lstrip()
s.rstrip()
s.split()
s.splitlines()</code></pre>
                        </div>
                        <div>
                            <h4>Search</h4>
                            <pre><code class="language-python">s.find(sub)
s.rfind(sub)
s.count(sub)
s.startswith()
s.endswith()</code></pre>
                        </div>
                    </div>
                    
                    <div class="tip-box fragment">
                        <h4>ğŸ’¡ When to Use What</h4>
                        <p><strong>String methods</strong>: fixed, literal operations<br>
                        <strong>Regex</strong>: patterns, multiple matches, complex extraction</p>
                    </div>
                    <aside class="notes">
                        String methods are faster for simple operations. Use regex for patterns.
                    </aside>
                </section>
                
                <!-- SLIDE 17: SPLIT AND JOIN -->
                <section>
                    <h2>Split and Join: Inverse Operations</h2>
                    
                    <pre><code class="language-python"># Split: string â†’ list
text = "apple,banana,cherry"
fruits = text.split(',')        # ['apple', 'banana', 'cherry']

# Join: list â†’ string
' | '.join(fruits)              # 'apple | banana | cherry'

# Split with limit
"a-b-c-d".split('-', 2)         # ['a', 'b', 'c-d']

# Split on whitespace (default)
"  word1   word2  ".split()     # ['word1', 'word2']

# Regex split for multiple delimiters
import re
re.split(r'[,;\s]+', "a, b; c d")  # ['a', 'b', 'c', 'd']</code></pre>
                    <aside class="notes">
                        Split and join are fundamental text operations. Note the whitespace handling differences.
                    </aside>
                </section>
                
                <!-- SLIDE 18: UNICODE -->
                <section>
                    <h2>Unicode Fundamentals</h2>
                    
                    <div class="concept-box">
                        <h4>ğŸ’¡ Key Concepts</h4>
                        <p><strong>Code point</strong>: Unique integer for each character (U+0041 = 'A')<br>
                        <strong>Encoding</strong>: How code points become bytes (UTF-8, UTF-16)<br>
                        <strong>Python strings</strong>: Sequences of code points (not bytes)</p>
                    </div>
                    
                    <pre><code class="language-python"># Code points
ord('A')        # 65
chr(65)         # 'A'
'\u0041'        # 'A' (Unicode escape)

# Encoding and decoding
text = "cafÃ©"
bytes_utf8 = text.encode('utf-8')     # b'caf\xc3\xa9'
bytes_utf16 = text.encode('utf-16')   # b'\xff\xfe...'

# Decoding
bytes_utf8.decode('utf-8')            # 'cafÃ©'</code></pre>
                    <aside class="notes">
                        Understanding Unicode prevents encoding errors. UTF-8 is the standard for text files.
                    </aside>
                </section>
                
                <!-- SLIDE 19: UNICODE NORMALISATION -->
                <section>
                    <h2>Unicode Normalisation</h2>
                    
                    <div class="warning-box">
                        <h4>âš ï¸ The Problem</h4>
                        <p>"cafÃ©" can be represented two ways:<br>
                        â€¢ Composed: <code>c a f Ã©</code> (4 code points)<br>
                        â€¢ Decomposed: <code>c a f e Ì</code> (5 code points: e + combining accent)<br>
                        These look identical but <code>==</code> returns <code>False</code>!</p>
                    </div>
                    
                    <pre><code class="language-python">import unicodedata

s1 = "cafÃ©"                          # precomposed
s2 = "cafe\u0301"                    # decomposed

s1 == s2                             # False!
len(s1), len(s2)                     # (4, 5)

# Normalise before comparison
unicodedata.normalize('NFC', s1) == \
unicodedata.normalize('NFC', s2)     # True</code></pre>
                    
                    <p class="small-text fragment">
                        <strong>NFC</strong>: Compose (prefer Ã©) | <strong>NFD</strong>: Decompose (prefer e + accent)
                    </p>
                    <aside class="notes">
                        Always normalise text before comparison or storage. NFC is typically appropriate.
                    </aside>
                </section>
            </section>
            
            <!-- ==================== PART III: NLP FUNDAMENTALS ==================== -->
            <section>
                <section>
                    <h1>PART III</h1>
                    <h2 style="color: #58a6ff;">NLP Fundamentals</h2>
                    <p class="small-text">From text to features</p>
                </section>
                
                <!-- SLIDE 20: NLP PIPELINE OVERVIEW -->
                <section>
                    <h2>The NLP Pipeline</h2>
                    
                    <div class="diagram-box">
<pre>
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                        TEXT PROCESSING PIPELINE                         â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚                                                                         â”‚
    â”‚   Raw Text                                                              â”‚
    â”‚      â”‚                                                                  â”‚
    â”‚      â–¼                                                                  â”‚
    â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
    â”‚   â”‚  TOKENISE    â”‚â”€â”€â–¶â”‚  NORMALISE   â”‚â”€â”€â–¶â”‚   FILTER     â”‚              â”‚
    â”‚   â”‚              â”‚   â”‚              â”‚   â”‚              â”‚              â”‚
    â”‚   â”‚ â€¢ words      â”‚   â”‚ â€¢ lowercase  â”‚   â”‚ â€¢ stopwords  â”‚              â”‚
    â”‚   â”‚ â€¢ sentences  â”‚   â”‚ â€¢ stem/lemma â”‚   â”‚ â€¢ punctuationâ”‚              â”‚
    â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
    â”‚                                              â”‚                         â”‚
    â”‚                                              â–¼                         â”‚
    â”‚                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
    â”‚                              â”‚    FEATURE EXTRACTION    â”‚              â”‚
    â”‚                              â”‚                          â”‚              â”‚
    â”‚                              â”‚  â€¢ bag-of-words          â”‚              â”‚
    â”‚                              â”‚  â€¢ n-grams               â”‚              â”‚
    â”‚                              â”‚  â€¢ TF-IDF                â”‚              â”‚
    â”‚                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
    â”‚                                              â”‚                         â”‚
    â”‚                                              â–¼                         â”‚
    â”‚                                      Feature Vectors                   â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
</pre>
                    </div>
                    <aside class="notes">
                        Text processing follows a standard pipeline. Each stage transforms its input for the next.
                    </aside>
                </section>
                
                <!-- SLIDE 21: DECOMPOSITION QUOTE -->
                <section>
                    <h2>Decomposition in Text Processing</h2>
                    
                    <div class="hook-box">
                        <p>
                            "Consider how decomposition might transform an approach to 
                            <strong>sentiment analysis</strong>â€”determining the emotional tone of a text. 
                            Rather than attempting to assess sentiment as a unified whole, a computational 
                            approach decomposes the task into distinct steps: preprocessing the text 
                            to normalize format, splitting it into sentences, analyzing sentiment at 
                            the word level, aggregating to determine sentence-level sentiment, and 
                            finally computing overall document sentiment."
                        </p>
                        <p class="source">â€” The Art of Computational Thinking for Researchers</p>
                    </div>
                    
                    <p class="fragment">
                        Each pipeline stage: <span class="highlight-text">single responsibility</span>, 
                        <span class="highlight-text">testable in isolation</span>.
                    </p>
                    <aside class="notes">
                        Decomposition makes complex text analysis tractable. Each stage can be tested and improved independently.
                    </aside>
                </section>
                
                <!-- SLIDE 22: TOKENISATION -->
                <section>
                    <h2>Tokenisation</h2>
                    
                    <p>Segmenting text into meaningful units:</p>
                    
                    <pre><code class="language-python">from nltk.tokenize import word_tokenize, sent_tokenize

text = "Dr. Smith's analysis won't be ready until 3:00 p.m."

# Word tokenisation
word_tokenize(text)
# ['Dr.', 'Smith', "'s", 'analysis', 'wo', "n't", 'be', 
#  'ready', 'until', '3:00', 'p.m', '.']

# Sentence tokenisation
sent_tokenize("Hello there! How are you? I'm fine.")
# ['Hello there!', 'How are you?', "I'm fine."]</code></pre>
                    
                    <div class="warning-box fragment">
                        <h4>âš ï¸ Tokenisation Challenges</h4>
                        <p>Contractions â€¢ Abbreviations â€¢ Hyphenated words â€¢ Numbers â€¢ URLs</p>
                    </div>
                    <aside class="notes">
                        Tokenisation is not trivial. NLTK handles many English edge cases.
                    </aside>
                </section>
                
                <!-- SLIDE 23: STEMMING -->
                <section>
                    <h2>Stemming: Rule-Based Reduction</h2>
                    
                    <pre><code class="language-python">from nltk.stem import PorterStemmer, SnowballStemmer

porter = PorterStemmer()
snowball = SnowballStemmer('english')

words = ['running', 'runs', 'ran', 'runner', 'easily']

for word in words:
    print(f"{word:12} â†’ {porter.stem(word)}")</code></pre>
                    
                    <div class="diagram-box">
<pre>
    running      â†’ run
    runs         â†’ run
    ran          â†’ ran        â† irregular not handled
    runner       â†’ runner
    easily       â†’ easili
</pre>
                    </div>
                    
                    <p class="small-text fragment">
                        <strong>Pros</strong>: Fast, no dictionary needed | 
                        <strong>Cons</strong>: May produce non-words, over-reduction
                    </p>
                    <aside class="notes">
                        Stemming applies rules to strip affixes. Fast but imperfect.
                    </aside>
                </section>
                
                <!-- SLIDE 24: LEMMATISATION -->
                <section>
                    <h2>Lemmatisation: Dictionary-Based</h2>
                    
                    <pre><code class="language-python">from nltk.stem import WordNetLemmatizer
from nltk.corpus import wordnet

lemmatiser = WordNetLemmatizer()

# Without POS: assumes noun
lemmatiser.lemmatize('running')         # 'running'

# With POS: verb
lemmatiser.lemmatize('running', pos='v') # 'run'

# Better results
lemmatiser.lemmatize('better', pos='a')  # 'good'
lemmatiser.lemmatize('was', pos='v')     # 'be'</code></pre>
                    
                    <div class="tip-box fragment">
                        <h4>ğŸ’¡ Stemming vs Lemmatisation</h4>
                        <p><strong>Stemming</strong>: Speed priority, information retrieval<br>
                        <strong>Lemmatisation</strong>: Accuracy priority, linguistic analysis</p>
                    </div>
                    <aside class="notes">
                        Lemmatisation uses dictionaries and produces valid words. Requires POS for accuracy.
                    </aside>
                </section>
                
                <!-- SLIDE 25: STEMMING VS LEMMATISATION COMPARISON -->
                <section>
                    <h2>Stemming vs Lemmatisation</h2>
                    
                    <table>
                        <tr><th>Word</th><th>Stem</th><th>Lemma</th></tr>
                        <tr><td>running</td><td>run</td><td>run (v)</td></tr>
                        <tr><td>better</td><td>better</td><td>good (a)</td></tr>
                        <tr><td>studies</td><td>studi</td><td>study</td></tr>
                        <tr><td>feet</td><td>feet</td><td>foot</td></tr>
                        <tr><td>university</td><td>univers</td><td>university</td></tr>
                        <tr><td>universe</td><td>univers</td><td>universe</td></tr>
                    </table>
                    
                    <p class="small-text fragment" style="margin-top: 15px;">
                        Note: <em>university</em> and <em>universe</em> stem identically despite different meanings
                    </p>
                    <aside class="notes">
                        The comparison illustrates trade-offs. Stemming conflates; lemmatisation preserves distinctions.
                    </aside>
                </section>
                
                <!-- SLIDE 26: STOPWORDS -->
                <section>
                    <h2>Stopword Removal</h2>
                    
                    <pre><code class="language-python">from nltk.corpus import stopwords

stop_words = set(stopwords.words('english'))
print(len(stop_words))  # 179 words

# Sample stopwords
# {'i', 'me', 'my', 'the', 'a', 'an', 'is', 'are', 'was', ...}

# Filtering
tokens = ['the', 'quick', 'brown', 'fox', 'is', 'running']
filtered = [t for t in tokens if t.lower() not in stop_words]
# ['quick', 'brown', 'fox', 'running']</code></pre>
                    
                    <div class="warning-box fragment">
                        <h4>âš ï¸ Domain Considerations</h4>
                        <p>Standard stopwords may remove domain-relevant terms.<br>
                        Legal: "shall", "hereby" | Medical: "patient", "treatment"</p>
                    </div>
                    <aside class="notes">
                        Stopwords reduce dimensionality but consider domain needs. Customise when necessary.
                    </aside>
                </section>
                
                <!-- SLIDE 27: POS TAGGING -->
                <section>
                    <h2>Part-of-Speech Tagging</h2>
                    
                    <pre><code class="language-python">import spacy

nlp = spacy.load('en_core_web_sm')
doc = nlp("The quick brown fox jumps over the lazy dog.")

for token in doc:
    print(f"{token.text:10} {token.pos_:6} {token.tag_}")</code></pre>
                    
                    <div class="diagram-box">
<pre>
    The        DET    DT     (determiner)
    quick      ADJ    JJ     (adjective)
    brown      ADJ    JJ     (adjective)
    fox        NOUN   NN     (singular noun)
    jumps      VERB   VBZ    (3rd person singular present)
    over       ADP    IN     (preposition)
    the        DET    DT     (determiner)
    lazy       ADJ    JJ     (adjective)
    dog        NOUN   NN     (singular noun)
    .          PUNCT  .      (punctuation)
</pre>
                    </div>
                    <aside class="notes">
                        POS tags enable syntax-aware processing. spaCy provides accurate tagging.
                    </aside>
                </section>
                
                <!-- SLIDE 28: N-GRAMS -->
                <section>
                    <h2>N-grams: Capturing Context</h2>
                    
                    <pre><code class="language-python">from nltk import ngrams

tokens = ['natural', 'language', 'processing', 'is', 'fun']

# Unigrams (n=1)
list(ngrams(tokens, 1))
# [('natural',), ('language',), ('processing',), ...]

# Bigrams (n=2)
list(ngrams(tokens, 2))
# [('natural', 'language'), ('language', 'processing'), 
#  ('processing', 'is'), ('is', 'fun')]

# Trigrams (n=3)
list(ngrams(tokens, 3))
# [('natural', 'language', 'processing'), 
#  ('language', 'processing', 'is'), ...]</code></pre>
                    
                    <p class="fragment">
                        N-grams capture <span class="highlight-text">local word sequences</span> and 
                        <span class="highlight-text">collocations</span>.
                    </p>
                    <aside class="notes">
                        N-grams preserve some word order information. Higher n means more context but more sparsity.
                    </aside>
                </section>
                
                <!-- SLIDE 29: BAG OF WORDS -->
                <section>
                    <h2>Bag-of-Words Representation</h2>
                    
                    <pre><code class="language-python">from collections import Counter

def bag_of_words(tokens: list[str]) -> dict[str, int]:
    """Convert token list to frequency dictionary."""
    return dict(Counter(tokens))

text1 = ['the', 'cat', 'sat', 'on', 'the', 'mat']
text2 = ['the', 'dog', 'sat', 'on', 'the', 'log']

bow1 = bag_of_words(text1)  # {'the': 2, 'cat': 1, 'sat': 1, ...}
bow2 = bag_of_words(text2)  # {'the': 2, 'dog': 1, 'sat': 1, ...}</code></pre>
                    
                    <div class="concept-box fragment">
                        <h4>ğŸ’¡ Key Property</h4>
                        <p>Bag-of-words <strong>ignores word order</strong>.<br>
                        "The cat sat on the mat" = "mat the on sat cat the"</p>
                    </div>
                    <aside class="notes">
                        BOW is simple but surprisingly effective. Order-independence is both limitation and feature.
                    </aside>
                </section>
                
                <!-- SLIDE 30: TF-IDF -->
                <section>
                    <h2>TF-IDF: Term Weighting</h2>
                    
                    <div class="concept-box">
                        <h4>ğŸ’¡ Intuition</h4>
                        <p>Words appearing often in <em>one</em> document but rarely <em>across</em> documents are distinctive.</p>
                    </div>
                    
                    <div class="two-columns">
                        <div>
                            <h4>Term Frequency (TF)</h4>
                            <p style="font-size: 0.8em;">How often term appears in document</p>
                            <p class="code-highlight">TF(t,d) = count(t,d) / |d|</p>
                        </div>
                        <div>
                            <h4>Inverse Document Freq (IDF)</h4>
                            <p style="font-size: 0.8em;">How rare term is across corpus</p>
                            <p class="code-highlight">IDF(t) = log(N / df(t))</p>
                        </div>
                    </div>
                    
                    <p class="fragment" style="text-align: center; margin-top: 20px;">
                        <span class="highlight-text" style="font-size: 1.2em;">TF-IDF(t,d) = TF(t,d) Ã— IDF(t)</span>
                    </p>
                    <aside class="notes">
                        TF-IDF balances local and global frequency. High scores indicate distinctive terms.
                    </aside>
                </section>
                
                <!-- SLIDE 31: TF-IDF EXAMPLE -->
                <section>
                    <h2>TF-IDF in Practice</h2>
                    
                    <pre><code class="language-python">from sklearn.feature_extraction.text import TfidfVectorizer

corpus = [
    "The cat sat on the mat.",
    "The dog sat on the log.",
    "The cat chased the dog."
]

vectoriser = TfidfVectorizer()
tfidf_matrix = vectoriser.fit_transform(corpus)

# Feature names (vocabulary)
print(vectoriser.get_feature_names_out())
# ['cat', 'chased', 'dog', 'log', 'mat', 'on', 'sat', 'the']

# TF-IDF values for document 0
print(tfidf_matrix[0].toarray())
# [[0.44, 0.0, 0.0, 0.0, 0.56, 0.35, 0.35, 0.50]]</code></pre>
                    
                    <p class="small-text fragment">
                        Note: "mat" has high TF-IDF (unique to doc 0), "the" has low (common everywhere)
                    </p>
                    <aside class="notes">
                        sklearn's TfidfVectorizer handles the computation. Inspect weights to understand document content.
                    </aside>
                </section>
                
                <!-- SLIDE 32: LIST COMPREHENSION QUOTE -->
                <section>
                    <h2>Functional Style in Text Processing</h2>
                    
                    <div class="hook-box">
                        <p>
                            "For example, a list comprehension like <code>[x**2 for x in range(10) if x % 2 == 0]</code> 
                            generates a list containing the squares of even numbers from 0 to 9, 
                            combining iteration, filtering, and transformation in a single expression."
                        </p>
                        <p class="source">â€” Hudak, 1989, p. 384</p>
                    </div>
                    
                    <pre class="fragment"><code class="language-python"># Text processing with comprehensions
tokens = ['The', 'Quick', 'Brown', 'FOX']

# Lowercase + filter short words
clean = [t.lower() for t in tokens if len(t) > 2]
# ['the', 'quick', 'brown', 'fox']

# Pipeline in one expression
processed = [lemmatiser.lemmatize(t.lower()) 
             for t in tokens 
             if t.lower() not in stop_words]</code></pre>
                    <aside class="notes">
                        List comprehensions embody the functional style common in text processing pipelines.
                    </aside>
                </section>
            </section>
            
            <!-- ==================== PART IV: PIPELINE DESIGN ==================== -->
            <section>
                <section>
                    <h1>PART IV</h1>
                    <h2 style="color: #58a6ff;">Pipeline Design</h2>
                    <p class="small-text">From components to systems</p>
                </section>
                
                <!-- SLIDE 33: TEXT CLEANING -->
                <section>
                    <h2>Text Cleaning Pipeline</h2>
                    
                    <pre><code class="language-python">import re
import unicodedata

def clean_text(text: str) -> str:
    """Apply standard cleaning transformations."""
    # Unicode normalisation
    text = unicodedata.normalize('NFC', text)
    
    # Remove HTML tags
    text = re.sub(r'<[^>]+>', '', text)
    
    # Normalise whitespace
    text = re.sub(r'\s+', ' ', text)
    
    # Remove non-printable characters
    text = ''.join(c for c in text if c.isprintable())
    
    return text.strip()</code></pre>
                    <aside class="notes">
                        Text cleaning removes noise. Each step addresses a specific issue.
                    </aside>
                </section>
                
                <!-- SLIDE 34: COMPLETE PIPELINE -->
                <section>
                    <h2>Complete Preprocessing Pipeline</h2>
                    
                    <pre><code class="language-python">from dataclasses import dataclass
from typing import Callable

@dataclass
class TextPipeline:
    """Configurable text preprocessing pipeline."""
    tokeniser: Callable[[str], list[str]]
    normaliser: Callable[[str], str]
    stopwords: set[str]
    
    def process(self, text: str) -> list[str]:
        """Execute the full pipeline."""
        # Tokenise
        tokens = self.tokeniser(text)
        
        # Normalise each token
        tokens = [self.normaliser(t) for t in tokens]
        
        # Filter stopwords and empty
        tokens = [t for t in tokens 
                  if t and t not in self.stopwords]
        
        return tokens</code></pre>
                    <aside class="notes">
                        A pipeline class encapsulates configuration and provides a consistent interface.
                    </aside>
                </section>
                
                <!-- SLIDE 35: RESEARCH APPLICATIONS -->
                <section>
                    <h2>Research Applications</h2>
                    
                    <table>
                        <tr><th>Domain</th><th>Application</th><th>Techniques</th></tr>
                        <tr>
                            <td>Literary Studies</td>
                            <td>Authorship attribution</td>
                            <td>N-grams, TF-IDF, stylometry</td>
                        </tr>
                        <tr>
                            <td>Social Science</td>
                            <td>Survey coding</td>
                            <td>Tokenisation, classification</td>
                        </tr>
                        <tr>
                            <td>History</td>
                            <td>OCR post-processing</td>
                            <td>Regex, fuzzy matching</td>
                        </tr>
                        <tr>
                            <td>Media Studies</td>
                            <td>Sentiment tracking</td>
                            <td>Lemmatisation, lexicons</td>
                        </tr>
                        <tr>
                            <td>Linguistics</td>
                            <td>Corpus analysis</td>
                            <td>POS, frequency analysis</td>
                        </tr>
                    </table>
                    <aside class="notes">
                        Text processing enables research across disciplines. Technique selection depends on the research question.
                    </aside>
                </section>
                
                <!-- SLIDE 36: DOMAIN SPECIFIC INDICES -->
                <section>
                    <h2>Specialised Text Structures</h2>
                    
                    <div class="hook-box">
                        <p>
                            "Domain-specific indices: Many research domains have developed 
                            specialized indexing structures optimized for particular data types. 
                            Examples include <strong>B-trees for database applications</strong>, 
                            <strong>suffix trees for text processing</strong>, and 
                            <strong>R-trees for spatial data</strong>."
                        </p>
                        <p class="source">â€” The Art of Computational Thinking for Researchers</p>
                    </div>
                    
                    <div class="tip-box fragment">
                        <h4>ğŸ’¡ When Scale Matters</h4>
                        <p>For large corpora (millions of documents), consider:<br>
                        â€¢ Inverted indices for search<br>
                        â€¢ Suffix arrays for substring matching<br>
                        â€¢ Locality-sensitive hashing for similarity</p>
                    </div>
                    <aside class="notes">
                        Efficient text search at scale requires specialised data structures beyond naive approaches.
                    </aside>
                </section>
            </section>
            
            <!-- ==================== PART V: LABORATORY PREVIEW ==================== -->
            <section>
                <section>
                    <h1>PART V</h1>
                    <h2 style="color: #58a6ff;">Laboratory Sessions</h2>
                    <p class="small-text">Hands-on implementation</p>
                </section>
                
                <!-- SLIDE 37: LAB 1 OVERVIEW -->
                <section>
                    <h2>Lab 11_01: Regex and String Operations</h2>
                    
                    <table>
                        <tr><th>Section</th><th>Focus</th><th>Lines</th></tr>
                        <tr><td>Â§1</td><td>String methods</td><td>~80</td></tr>
                        <tr><td>Â§2</td><td>Regex fundamentals</td><td>~120</td></tr>
                        <tr><td>Â§3</td><td>Advanced patterns</td><td>~120</td></tr>
                        <tr><td>Â§4</td><td>Unicode handling</td><td>~80</td></tr>
                        <tr><td>Â§5</td><td>Text cleaning pipeline</td><td>~100</td></tr>
                    </table>
                    
                    <p style="margin-top: 15px;">
                        <strong>Duration</strong>: 50 minutes | 
                        <strong>Learning Objectives</strong>: LO1, LO2
                    </p>
                    <aside class="notes">
                        Lab 1 covers regex and string manipulation. Five progressive sections.
                    </aside>
                </section>
                
                <!-- SLIDE 38: LAB 2 OVERVIEW -->
                <section>
                    <h2>Lab 11_02: NLP Fundamentals</h2>
                    
                    <table>
                        <tr><th>Section</th><th>Focus</th><th>Lines</th></tr>
                        <tr><td>Â§1</td><td>Tokenisation</td><td>~80</td></tr>
                        <tr><td>Â§2</td><td>Normalisation</td><td>~80</td></tr>
                        <tr><td>Â§3</td><td>Text features</td><td>~100</td></tr>
                        <tr><td>Â§4</td><td>Basic NLP tasks</td><td>~90</td></tr>
                    </table>
                    
                    <p style="margin-top: 15px;">
                        <strong>Duration</strong>: 40 minutes | 
                        <strong>Learning Objectives</strong>: LO3, LO4, LO5
                    </p>
                    <aside class="notes">
                        Lab 2 introduces NLP with NLTK and spaCy. Builds toward feature extraction.
                    </aside>
                </section>
                
                <!-- SLIDE 39: EXERCISES OVERVIEW -->
                <section>
                    <h2>Practice Exercises</h2>
                    
                    <div class="three-columns">
                        <div>
                            <h4 style="color: var(--success-green);">Easy</h4>
                            <p class="small-text">10-15 min each</p>
                            <ul style="font-size: 0.7em;">
                                <li>String operations</li>
                                <li>Regex basics</li>
                                <li>Tokenisation intro</li>
                            </ul>
                        </div>
                        <div>
                            <h4 style="color: var(--warning-orange);">Medium</h4>
                            <p class="small-text">20-25 min each</p>
                            <ul style="font-size: 0.7em;">
                                <li>Regex extraction</li>
                                <li>Text normalisation</li>
                                <li>Frequency analysis</li>
                            </ul>
                        </div>
                        <div>
                            <h4 style="color: var(--warning-red);">Hard</h4>
                            <p class="small-text">30-40 min each</p>
                            <ul style="font-size: 0.7em;">
                                <li>Document parser</li>
                                <li>NLP pipeline</li>
                                <li>Corpus analyser</li>
                            </ul>
                        </div>
                    </div>
                    <aside class="notes">
                        Nine exercises at three difficulty levels. Start with easy, progress as comfortable.
                    </aside>
                </section>
            </section>
            
            <!-- ==================== SUMMARY ==================== -->
            <section>
                <!-- SLIDE 40: KEY TAKEAWAYS -->
                <section>
                    <h2>ğŸ“Œ Key Takeaways</h2>
                    
                    <div class="two-columns">
                        <div>
                            <h4>Regular Expressions</h4>
                            <ul style="font-size: 0.8em;">
                                <li>Metacharacters: <code>. ^ $ * + ?</code></li>
                                <li>Character classes: <code>\d \w \s</code></li>
                                <li>Groups for extraction</li>
                                <li>Lookahead/lookbehind</li>
                            </ul>
                        </div>
                        <div>
                            <h4>NLP Fundamentals</h4>
                            <ul style="font-size: 0.8em;">
                                <li>Tokenisation strategies</li>
                                <li>Stemming vs lemmatisation</li>
                                <li>Stopword filtering</li>
                                <li>TF-IDF weighting</li>
                            </ul>
                        </div>
                    </div>
                    
                    <div class="concept-box fragment" style="margin-top: 20px;">
                        <h4>ğŸ’¡ Core Insight</h4>
                        <p>Text processing transforms unstructured character sequences into structured representations suitable for computational analysis.</p>
                    </div>
                    <aside class="notes">
                        Summary of key concepts. These form the foundation for ML text features in Unit 13.
                    </aside>
                </section>
                
                <!-- SLIDE 41: COMMON PITFALLS -->
                <section>
                    <h2>âš ï¸ Common Pitfalls</h2>
                    
                    <div class="warning-box">
                        <h4>Regex Gotchas</h4>
                        <ul style="font-size: 0.8em;">
                            <li>Greedy matching: use <code>*?</code> for non-greedy</li>
                            <li>Forgetting raw strings: <code>r'\d+'</code> not <code>'\d+'</code></li>
                            <li>Anchors in multiline mode</li>
                        </ul>
                    </div>
                    
                    <div class="warning-box">
                        <h4>NLP Gotchas</h4>
                        <ul style="font-size: 0.8em;">
                            <li>Tokenisation assumptions (English â‰  Chinese)</li>
                            <li>Stemmer over-reduction (university = universe)</li>
                            <li>Standard stopwords may be domain-inappropriate</li>
                        </ul>
                    </div>
                    <aside class="notes">
                        Awareness of common pitfalls prevents debugging frustration.
                    </aside>
                </section>
                
                <!-- SLIDE 42: NEXT STEPS -->
                <section>
                    <h2>ğŸš€ Next Steps</h2>
                    
                    <ol>
                        <li class="fragment">Complete <strong>Lab 11_01</strong> (regex and strings)</li>
                        <li class="fragment">Work through <strong>easy exercises</strong></li>
                        <li class="fragment">Complete <strong>Lab 11_02</strong> (NLP fundamentals)</li>
                        <li class="fragment">Attempt <strong>medium and hard exercises</strong></li>
                        <li class="fragment">Take the <strong>self-assessment quiz</strong></li>
                    </ol>
                    
                    <div class="tip-box fragment" style="margin-top: 20px;">
                        <h4>ğŸ’¡ Preparation for 13UNIT</h4>
                        <p>The text features you learn here (TF-IDF, n-grams) become input representations for machine learning classifiers.</p>
                    </div>
                    <aside class="notes">
                        Follow the recommended sequence. Skills build toward ML applications.
                    </aside>
                </section>
                
                <!-- SLIDE 43: RESOURCES -->
                <section>
                    <h2>ğŸ“š Resources</h2>
                    
                    <div class="two-columns">
                        <div>
                            <h4>Documentation</h4>
                            <ul style="font-size: 0.75em;">
                                <li><a href="https://docs.python.org/3/library/re.html">Python re module</a></li>
                                <li><a href="https://www.nltk.org/">NLTK Documentation</a></li>
                                <li><a href="https://spacy.io/usage">spaCy Usage Guide</a></li>
                            </ul>
                        </div>
                        <div>
                            <h4>Interactive Tools</h4>
                            <ul style="font-size: 0.75em;">
                                <li><a href="https://regex101.com/">regex101.com</a></li>
                                <li><a href="https://regexr.com/">RegExr</a></li>
                                <li><code>11UNIT_regex_tester.html</code></li>
                            </ul>
                        </div>
                    </div>
                    
                    <p class="small-text" style="margin-top: 30px; text-align: center;">
                        See <code>resources/further_reading.md</code> for annotated bibliography
                    </p>
                    <aside class="notes">
                        External resources for deeper exploration. Regex testers are invaluable for learning.
                    </aside>
                </section>
                
                <!-- SLIDE 44: QUESTIONS -->
                <section data-background-gradient="linear-gradient(135deg, #0d1117 0%, #161b22 100%)">
                    <h1>â“ Questions?</h1>
                    
                    <p style="color: #8b949e; margin-top: 40px;">
                        Text Processing and NLP Fundamentals<br>
                        Unit 11 of 14
                    </p>
                    
                    <hr style="border-color: #30363d; margin: 40px 0;">
                    
                    <p style="font-size: 0.8em;">
                        Next: <span class="highlight-text">Laboratory Sessions</span>
                    </p>
                    <aside class="notes">
                        Time for questions before we proceed to hands-on work.
                    </aside>
                </section>
            </section>
            
        </div>
    </div>
    
    <script src="https://cdn.jsdelivr.net/npm/reveal.js@5.0.4/dist/reveal.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/reveal.js@5.0.4/plugin/highlight/highlight.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/reveal.js@5.0.4/plugin/notes/notes.js"></script>
    <script>
        Reveal.initialize({
            hash: true,
            slideNumber: 'c/t',
            showSlideNumber: 'all',
            transition: 'slide',
            transitionSpeed: 'default',
            backgroundTransition: 'fade',
            plugins: [RevealHighlight, RevealNotes],
            
            // Responsive settings
            width: '100%',
            height: '100%',
            margin: 0.04,
            minScale: 0.2,
            maxScale: 2.0
        });
    </script>
</body>
</html>
