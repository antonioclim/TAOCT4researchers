# Rubric — 11UNIT

## Criteria

Assessment focuses on evidence of reasoning and reproducibility is treated here as a research-
oriented craft rather than a collection of library calls. The central question is how an analyst
moves from raw character sequences to defensible claims, given that token boundaries, normalisation
choices and annotation schemes impose inductive bias. We therefore separate specification from
implementation: we first state what a pipeline must preserve or discard, then encode those
constraints as pure functions with explicit preconditions. Attention is paid to failure modes,
including ambiguous segmentation, encoding artefacts and domain drift, since these frequently
dominate error budgets in empirical studies. In practice, the unit couples formal definitions with
executable checks so that each step has a measurable contract: input alphabet, transformation
invariants and complexity expectations.

Assessment focuses on evidence of reasoning and reproducibility is treated here as a research-
oriented craft rather than a collection of library calls. The central question is how an analyst
moves from raw character sequences to defensible claims, given that token boundaries, normalisation
choices and annotation schemes impose inductive bias. We therefore separate specification from
implementation: we first state what a pipeline must preserve or discard, then encode those
constraints as pure functions with explicit preconditions. Attention is paid to failure modes,
including ambiguous segmentation, encoding artefacts and domain drift, since these frequently
dominate error budgets in empirical studies. In practice, the unit couples formal definitions with
executable checks so that each step has a measurable contract: input alphabet, transformation
invariants and complexity expectations.

Assessment focuses on evidence of reasoning and reproducibility is treated here as a research-
oriented craft rather than a collection of library calls. The central question is how an analyst
moves from raw character sequences to defensible claims, given that token boundaries, normalisation
choices and annotation schemes impose inductive bias. We therefore separate specification from
implementation: we first state what a pipeline must preserve or discard, then encode those
constraints as pure functions with explicit preconditions. Attention is paid to failure modes,
including ambiguous segmentation, encoding artefacts and domain drift, since these frequently
dominate error budgets in empirical studies. In practice, the unit couples formal definitions with
executable checks so that each step has a measurable contract: input alphabet, transformation
invariants and complexity expectations.

Assessment focuses on evidence of reasoning and reproducibility is treated here as a research-
oriented craft rather than a collection of library calls. The central question is how an analyst
moves from raw character sequences to defensible claims, given that token boundaries, normalisation
choices and annotation schemes impose inductive bias. We therefore separate specification from
implementation: we first state what a pipeline must preserve or discard, then encode those
constraints as pure functions with explicit preconditions. Attention is paid to failure modes,
including ambiguous segmentation, encoding artefacts and domain drift, since these frequently
dominate error budgets in empirical studies. In practice, the unit couples formal definitions with
executable checks so that each step has a measurable contract: input alphabet, transformation
invariants and complexity expectations.

Assessment focuses on evidence of reasoning and reproducibility is treated here as a research-
oriented craft rather than a collection of library calls. The central question is how an analyst
moves from raw character sequences to defensible claims, given that token boundaries, normalisation
choices and annotation schemes impose inductive bias. We therefore separate specification from
implementation: we first state what a pipeline must preserve or discard, then encode those
constraints as pure functions with explicit preconditions. Attention is paid to failure modes,
including ambiguous segmentation, encoding artefacts and domain drift, since these frequently
dominate error budgets in empirical studies. In practice, the unit couples formal definitions with
executable checks so that each step has a measurable contract: input alphabet, transformation
invariants and complexity expectations.

Assessment focuses on evidence of reasoning and reproducibility is treated here as a research-
oriented craft rather than a collection of library calls. The central question is how an analyst
moves from raw character sequences to defensible claims, given that token boundaries, normalisation
choices and annotation schemes impose inductive bias. We therefore separate specification from
implementation: we first state what a pipeline must preserve or discard, then encode those
constraints as pure functions with explicit preconditions. Attention is paid to failure modes,
including ambiguous segmentation, encoding artefacts and domain drift, since these frequently
dominate error budgets in empirical studies. In practice, the unit couples formal definitions with
executable checks so that each step has a measurable contract: input alphabet, transformation
invariants and complexity expectations.

## Marking grid

| Component | Excellent (A) | Good (B) | Satisfactory (C) | Weak (D/E) |
|---|---|---|---|---|
| Data handling | Clear encoding audit, documented transformations | Minor omissions, generally correct | Basic handling, limited documentation | Errors in encoding or missing evidence |
| Regex extraction | Tested patterns, quantified errors, edge cases covered | Tested patterns, limited edge cases | Works on typical cases | Fragile patterns, no tests |
| Corpus statistics | Correct computation and interpretation | Correct computation, limited interpretation | Some correct statistics | Incorrect statistics or interpretation |
| TF–IDF modelling | Reproducible pipeline, evaluation and error analysis | Evaluation present, limited analysis | Minimal evaluation | Missing evaluation or flawed setup |
| Writing | Precise, structured, sources cited | Mostly clear, minor issues | Understandable but uneven | Unclear, missing structure |

Academic integrity is treated here as a research-oriented craft rather than a collection of library
calls. The central question is how an analyst moves from raw character sequences to defensible
claims, given that token boundaries, normalisation choices and annotation schemes impose inductive
bias. We therefore separate specification from implementation: we first state what a pipeline must
preserve or discard, then encode those constraints as pure functions with explicit preconditions.
Attention is paid to failure modes, including ambiguous segmentation, encoding artefacts and domain
drift, since these frequently dominate error budgets in empirical studies. In practice, the unit
couples formal definitions with executable checks so that each step has a measurable contract: input
alphabet, transformation invariants and complexity expectations.

Academic integrity is treated here as a research-oriented craft rather than a collection of library
calls. The central question is how an analyst moves from raw character sequences to defensible
claims, given that token boundaries, normalisation choices and annotation schemes impose inductive
bias. We therefore separate specification from implementation: we first state what a pipeline must
preserve or discard, then encode those constraints as pure functions with explicit preconditions.
Attention is paid to failure modes, including ambiguous segmentation, encoding artefacts and domain
drift, since these frequently dominate error budgets in empirical studies. In practice, the unit
couples formal definitions with executable checks so that each step has a measurable contract: input
alphabet, transformation invariants and complexity expectations.

Academic integrity is treated here as a research-oriented craft rather than a collection of library
calls. The central question is how an analyst moves from raw character sequences to defensible
claims, given that token boundaries, normalisation choices and annotation schemes impose inductive
bias. We therefore separate specification from implementation: we first state what a pipeline must
preserve or discard, then encode those constraints as pure functions with explicit preconditions.
Attention is paid to failure modes, including ambiguous segmentation, encoding artefacts and domain
drift, since these frequently dominate error budgets in empirical studies. In practice, the unit
couples formal definitions with executable checks so that each step has a measurable contract: input
alphabet, transformation invariants and complexity expectations.

Academic integrity is treated here as a research-oriented craft rather than a collection of library
calls. The central question is how an analyst moves from raw character sequences to defensible
claims, given that token boundaries, normalisation choices and annotation schemes impose inductive
bias. We therefore separate specification from implementation: we first state what a pipeline must
preserve or discard, then encode those constraints as pure functions with explicit preconditions.
Attention is paid to failure modes, including ambiguous segmentation, encoding artefacts and domain
drift, since these frequently dominate error budgets in empirical studies. In practice, the unit
couples formal definitions with executable checks so that each step has a measurable contract: input
alphabet, transformation invariants and complexity expectations.
