# Cheatsheet — 13UNIT

## Pipeline essentials

`Pipeline([...])` ensures transformations are fitted on training data only.

`cross_val_score` estimates performance under K-fold splits; record the splitter and random state.

For class imbalance, inspect the confusion matrix and prefer precision–recall measures.



## Common metrics

Classification: accuracy, balanced accuracy, precision, recall, F1, ROC AUC, PR AUC.

Regression: mean squared error, mean absolute error, $R^2$ with an explicit baseline.



## Diagnostics

Learning curves: training and validation error versus sample size.

Calibration: reliability curves for probabilistic classifiers.




From a methodological perspective, the central difficulty is not writing code but specifying the inferential claim, the assumptions under which it is defensible and the diagnostics that reveal when those assumptions fail.

Accordingly, each experiment should be accompanied by a record of data provenance, preprocessing choices and metric definitions, because these decisions often dominate model selection effects when datasets are modest in size.

In applied work, it is prudent to treat performance estimates as random variables conditioned on a sampling design; resampling methods and confidence intervals are therefore discussed alongside point estimates.

From a methodological perspective, the central difficulty is not writing code but specifying the inferential claim, the assumptions under which it is defensible and the diagnostics that reveal when those assumptions fail.

Accordingly, each experiment should be accompanied by a record of data provenance, preprocessing choices and metric definitions, because these decisions often dominate model selection effects when datasets are modest in size.

In applied work, it is prudent to treat performance estimates as random variables conditioned on a sampling design; resampling methods and confidence intervals are therefore discussed alongside point estimates.

From a methodological perspective, the central difficulty is not writing code but specifying the inferential claim, the assumptions under which it is defensible and the diagnostics that reveal when those assumptions fail.

Accordingly, each experiment should be accompanied by a record of data provenance, preprocessing choices and metric definitions, because these decisions often dominate model selection effects when datasets are modest in size.

In applied work, it is prudent to treat performance estimates as random variables conditioned on a sampling design; resampling methods and confidence intervals are therefore discussed alongside point estimates.

From a methodological perspective, the central difficulty is not writing code but specifying the inferential claim, the assumptions under which it is defensible and the diagnostics that reveal when those assumptions fail.

Accordingly, each experiment should be accompanied by a record of data provenance, preprocessing choices and metric definitions, because these decisions often dominate model selection effects when datasets are modest in size.

In applied work, it is prudent to treat performance estimates as random variables conditioned on a sampling design; resampling methods and confidence intervals are therefore discussed alongside point estimates.

From a methodological perspective, the central difficulty is not writing code but specifying the inferential claim, the assumptions under which it is defensible and the diagnostics that reveal when those assumptions fail.

Accordingly, each experiment should be accompanied by a record of data provenance, preprocessing choices and metric definitions, because these decisions often dominate model selection effects when datasets are modest in size.

In applied work, it is prudent to treat performance estimates as random variables conditioned on a sampling design; resampling methods and confidence intervals are therefore discussed alongside point estimates.

From a methodological perspective, the central difficulty is not writing code but specifying the inferential claim, the assumptions under which it is defensible and the diagnostics that reveal when those assumptions fail.

Accordingly, each experiment should be accompanied by a record of data provenance, preprocessing choices and metric definitions, because these decisions often dominate model selection effects when datasets are modest in size.

In applied work, it is prudent to treat performance estimates as random variables conditioned on a sampling design; resampling methods and confidence intervals are therefore discussed alongside point estimates.

From a methodological perspective, the central difficulty is not writing code but specifying the inferential claim, the assumptions under which it is defensible and the diagnostics that reveal when those assumptions fail.

Accordingly, each experiment should be accompanied by a record of data provenance, preprocessing choices and metric definitions, because these decisions often dominate model selection effects when datasets are modest in size.

In applied work, it is prudent to treat performance estimates as random variables conditioned on a sampling design; resampling methods and confidence intervals are therefore discussed alongside point estimates.

From a methodological perspective, the central difficulty is not writing code but specifying the inferential claim, the assumptions under which it is defensible and the diagnostics that reveal when those assumptions fail.

Accordingly, each experiment should be accompanied by a record of data provenance, preprocessing choices and metric definitions, because these decisions often dominate model selection effects when datasets are modest in size.

In applied work, it is prudent to treat performance estimates as random variables conditioned on a sampling design; resampling methods and confidence intervals are therefore discussed alongside point estimates.

From a methodological perspective, the central difficulty is not writing code but specifying the inferential claim, the assumptions under which it is defensible and the diagnostics that reveal when those assumptions fail.

Accordingly, each experiment should be accompanied by a record of data provenance, preprocessing choices and metric definitions, because these decisions often dominate model selection effects when datasets are modest in size.

In applied work, it is prudent to treat performance estimates as random variables conditioned on a sampling design; resampling methods and confidence intervals are therefore discussed alongside point estimates.