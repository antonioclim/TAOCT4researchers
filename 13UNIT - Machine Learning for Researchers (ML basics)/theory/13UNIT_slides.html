<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>13UNIT: Machine Learning for Researchers</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/5.0.4/reveal.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/5.0.4/theme/night.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/monokai.min.css">
    <style>
        :root {
            --r-heading-color: #4a9eff;
            --r-link-color: #4ecdc4;
        }
        .reveal h1, .reveal h2 { color: var(--r-heading-color); }
        .reveal pre { font-size: 0.55em; }
        .reveal .slides section { text-align: left; }
        .reveal .slides section.center { text-align: center; }
        .two-column { display: flex; gap: 2em; }
        .two-column > div { flex: 1; }
        .highlight-box { background: rgba(74, 158, 255, 0.1); padding: 1em; border-radius: 8px; border-left: 4px solid #4a9eff; }
        .warning-box { background: rgba(255, 107, 107, 0.1); padding: 1em; border-radius: 8px; border-left: 4px solid #ff6b6b; }
        .success-box { background: rgba(78, 205, 196, 0.1); padding: 1em; border-radius: 8px; border-left: 4px solid #4ecdc4; }
        .small { font-size: 0.7em; }
        table { width: 100%; border-collapse: collapse; font-size: 0.8em; }
        th, td { border: 1px solid #444; padding: 0.5em; text-align: left; }
        th { background: rgba(74, 158, 255, 0.2); }
    </style>
</head>
<body>
<div class="reveal">
<div class="slides">

<!-- SECTION 1: Introduction (Slides 1-5) -->
<section class="center">
    <h1>Machine Learning for Researchers</h1>
    <h3>UNIT 13</h3>
    <p>Academy of Economic Studies • Bucharest</p>
    <p class="small">Duration: 10-12 hours • Difficulty: ★★★★★</p>
    <aside class="notes">Welcome to Unit 13 on Machine Learning fundamentals. This unit bridges theoretical understanding with practical implementation using scikit-learn.</aside>
</section>

<section>
    <h2>Learning Objectives</h2>
    <ol>
        <li>Distinguish supervised, unsupervised, and reinforcement learning paradigms</li>
        <li>Implement classification and regression pipelines using scikit-learn</li>
        <li>Apply proper validation: train/test split, cross-validation, stratification</li>
        <li>Select and interpret metrics appropriate for problem types</li>
        <li>Identify and mitigate pitfalls: overfitting, data leakage, class imbalance</li>
        <li>Implement unsupervised learning: clustering and dimensionality reduction</li>
    </ol>
    <aside class="notes">These six objectives span both conceptual understanding and hands-on implementation skills.</aside>
</section>

<section>
    <h2>Prerequisites</h2>
    <div class="two-column">
        <div>
            <h4>Required Knowledge</h4>
            <ul>
                <li>Python fundamentals (06UNIT)</li>
                <li>NumPy array operations</li>
                <li>Data visualisation basics</li>
                <li>Basic statistics</li>
            </ul>
        </div>
        <div>
            <h4>This Unit Prepares For</h4>
            <ul>
                <li>14UNIT: Parallel ML Training</li>
                <li>Advanced deep learning</li>
                <li>Research applications</li>
            </ul>
        </div>
    </div>
    <aside class="notes">Ensure familiarity with NumPy before proceeding. Statistics knowledge helps with metric interpretation.</aside>
</section>

<section>
    <h2>What is Machine Learning?</h2>
    <div class="highlight-box">
        <p>"A computer program is said to learn from experience E with respect to some task T and performance measure P, if its performance at T, as measured by P, improves with experience E."</p>
        <p class="small">— Tom Mitchell, 1997</p>
    </div>
    <p style="margin-top: 1em;">ML enables systems to <strong>learn patterns from data</strong> rather than being explicitly programmed.</p>
    <aside class="notes">Mitchell's definition remains the standard. The key insight: learning improves with more data/experience.</aside>
</section>

<section>
    <h2>Models as Abstractions</h2>
    <blockquote class="small">
        "Consider a physicist trying to model a bouncing ball. The physicist knows that the ball's behaviour depends on its mass, elasticity, air resistance, and the surface it bounces on. Rather than simulating every molecule in the ball... the physicist creates a mathematical model..."
    </blockquote>
    <p>ML models similarly capture essential patterns whilst abstracting away irrelevant details.</p>
    <aside class="notes">Book Extract 13-A. Models balance fidelity against complexity - more detail isn't always better.</aside>
</section>

<!-- SECTION 2: Learning Paradigms (Slides 6-12) -->
<section class="center">
    <h2>Learning Paradigms</h2>
    <aside class="notes">Now we examine the three major ML paradigms.</aside>
</section>

<section>
    <h2>Supervised Learning</h2>
    <div class="two-column">
        <div>
            <h4>Characteristics</h4>
            <ul>
                <li>Labelled training data (X, y)</li>
                <li>Learn mapping f: X → y</li>
                <li>Predict labels for new data</li>
            </ul>
            <h4>Types</h4>
            <ul>
                <li><strong>Classification</strong>: discrete labels</li>
                <li><strong>Regression</strong>: continuous values</li>
            </ul>
        </div>
        <div>
            <h4>Examples</h4>
            <ul>
                <li>Spam detection</li>
                <li>Medical diagnosis</li>
                <li>House price prediction</li>
                <li>Customer churn</li>
            </ul>
        </div>
    </div>
    <aside class="notes">Supervised learning requires labelled data - often the most expensive component.</aside>
</section>

<section>
    <h2>Unsupervised Learning</h2>
    <div class="two-column">
        <div>
            <h4>Characteristics</h4>
            <ul>
                <li>Unlabelled data (X only)</li>
                <li>Discover hidden structure</li>
                <li>No "correct" answers</li>
            </ul>
            <h4>Types</h4>
            <ul>
                <li><strong>Clustering</strong>: group similar items</li>
                <li><strong>Dimensionality Reduction</strong>: compress features</li>
            </ul>
        </div>
        <div>
            <h4>Examples</h4>
            <ul>
                <li>Customer segmentation</li>
                <li>Anomaly detection</li>
                <li>Topic modelling</li>
                <li>Data visualisation</li>
            </ul>
        </div>
    </div>
    <aside class="notes">Unsupervised learning finds patterns without human-provided labels.</aside>
</section>

<section>
    <h2>Reinforcement Learning</h2>
    <div class="two-column">
        <div>
            <h4>Characteristics</h4>
            <ul>
                <li>Agent interacts with environment</li>
                <li>Receives rewards/penalties</li>
                <li>Learns optimal policy</li>
            </ul>
        </div>
        <div>
            <h4>Examples</h4>
            <ul>
                <li>Game playing (AlphaGo)</li>
                <li>Robotics control</li>
                <li>Recommendation systems</li>
                <li>Resource management</li>
            </ul>
        </div>
    </div>
    <div class="highlight-box" style="margin-top: 1em;">
        <p class="small">RL is beyond this unit's scope but understanding its paradigm helps position supervised/unsupervised methods.</p>
    </div>
    <aside class="notes">RL requires an environment for interaction - distinct from static datasets.</aside>
</section>

<section>
    <h2>Paradigm Comparison</h2>
    <table>
        <tr>
            <th>Aspect</th>
            <th>Supervised</th>
            <th>Unsupervised</th>
            <th>Reinforcement</th>
        </tr>
        <tr>
            <td>Data</td>
            <td>Labelled (X, y)</td>
            <td>Unlabelled (X)</td>
            <td>States, actions, rewards</td>
        </tr>
        <tr>
            <td>Goal</td>
            <td>Predict labels</td>
            <td>Find structure</td>
            <td>Maximise reward</td>
        </tr>
        <tr>
            <td>Evaluation</td>
            <td>Clear metrics</td>
            <td>Subjective</td>
            <td>Cumulative reward</td>
        </tr>
        <tr>
            <td>Feedback</td>
            <td>Immediate</td>
            <td>None</td>
            <td>Delayed</td>
        </tr>
    </table>
    <aside class="notes">This comparison helps select the right approach for a given problem.</aside>
</section>

<section>
    <h2>Choosing a Paradigm</h2>
    <pre><code class="language-text">Do you have labelled data?
├── Yes → Supervised Learning
│   ├── Discrete labels? → Classification
│   └── Continuous values? → Regression
├── No, but have data → Unsupervised Learning
│   ├── Group similar items? → Clustering
│   └── Reduce dimensions? → PCA, t-SNE
└── No, but have environment → Reinforcement Learning
    └── Sequential decisions? → Policy learning</code></pre>
    <aside class="notes">Decision tree for paradigm selection based on data availability.</aside>
</section>

<!-- SECTION 3: scikit-learn (Slides 13-18) -->
<section class="center">
    <h2>The scikit-learn Ecosystem</h2>
    <aside class="notes">scikit-learn provides a consistent API for ML in Python.</aside>
</section>

<section>
    <h2>The Estimator API</h2>
    <p>All scikit-learn models follow a consistent interface:</p>
    <pre><code class="language-python">from sklearn.linear_model import LogisticRegression

# Create estimator
model = LogisticRegression()

# Fit to training data
model.fit(X_train, y_train)

# Make predictions
y_pred = model.predict(X_test)

# Get probabilities (classifiers)
y_prob = model.predict_proba(X_test)</code></pre>
    <div class="success-box">
        <p>Key methods: <code>fit()</code>, <code>predict()</code>, <code>transform()</code>, <code>fit_transform()</code></p>
    </div>
    <aside class="notes">This consistent API means learning one estimator teaches you all of them.</aside>
</section>

<section>
    <h2>Common Classifiers</h2>
    <table>
        <tr>
            <th>Algorithm</th>
            <th>Import</th>
            <th>Key Parameters</th>
        </tr>
        <tr>
            <td>Logistic Regression</td>
            <td><code>LogisticRegression</code></td>
            <td>C, max_iter</td>
        </tr>
        <tr>
            <td>Decision Tree</td>
            <td><code>DecisionTreeClassifier</code></td>
            <td>max_depth</td>
        </tr>
        <tr>
            <td>Random Forest</td>
            <td><code>RandomForestClassifier</code></td>
            <td>n_estimators</td>
        </tr>
        <tr>
            <td>k-Nearest Neighbours</td>
            <td><code>KNeighborsClassifier</code></td>
            <td>n_neighbors</td>
        </tr>
        <tr>
            <td>Support Vector Machine</td>
            <td><code>SVC</code></td>
            <td>C, kernel</td>
        </tr>
    </table>
    <aside class="notes">Each algorithm has strengths for different data characteristics.</aside>
</section>

<section>
    <h2>Pipelines</h2>
    <p>Combine preprocessing and modelling in a single object:</p>
    <pre><code class="language-python">from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression

pipeline = Pipeline([
    ('scaler', StandardScaler()),
    ('classifier', LogisticRegression())
])

# Fit entire pipeline
pipeline.fit(X_train, y_train)

# Predict (applies all transformations)
predictions = pipeline.predict(X_test)</code></pre>
    <div class="warning-box">
        <p><strong>Critical:</strong> Pipelines prevent data leakage by ensuring preprocessing is fit only on training data.</p>
    </div>
    <aside class="notes">Pipelines are essential for reproducible, leak-free workflows.</aside>
</section>

<section>
    <h2>ColumnTransformer</h2>
    <p>Apply different preprocessing to different feature types:</p>
    <pre><code class="language-python">from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import StandardScaler, OneHotEncoder

preprocessor = ColumnTransformer([
    ('num', StandardScaler(), [0, 1, 2]),      # Numerical
    ('cat', OneHotEncoder(), [3, 4])           # Categorical
])

# In a pipeline
full_pipeline = Pipeline([
    ('preprocess', preprocessor),
    ('classifier', LogisticRegression())
])</code></pre>
    <aside class="notes">Real datasets often have mixed types requiring different preprocessing.</aside>
</section>

<!-- SECTION 4: Validation (Slides 19-26) -->
<section class="center">
    <h2>Validation Methodology</h2>
    <aside class="notes">Proper validation is essential for trustworthy performance estimates.</aside>
</section>

<section>
    <h2>Train/Test Split</h2>
    <div class="two-column">
        <div>
            <pre><code class="language-python">from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    X, y,
    test_size=0.2,        # 20% for testing
    random_state=42,      # Reproducibility
    stratify=y            # Preserve class proportions
)</code></pre>
        </div>
        <div>
            <div class="highlight-box">
                <h4>Key Points</h4>
                <ul>
                    <li>Never train on test data</li>
                    <li>Use stratification for classification</li>
                    <li>Set random_state for reproducibility</li>
                </ul>
            </div>
        </div>
    </div>
    <aside class="notes">The test set must remain untouched until final evaluation.</aside>
</section>

<section>
    <h2>Why Stratification?</h2>
    <pre><code class="language-python"># Without stratification - may get unbalanced splits
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
# Risk: test set might have 10% class A, training has 50%

# With stratification - proportions preserved
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, stratify=y
)
# Both sets have same class proportions as original</code></pre>
    <div class="success-box">
        <p>Stratification ensures representative splits, especially important for imbalanced classes.</p>
    </div>
    <aside class="notes">Without stratification, you might train on one distribution and test on another.</aside>
</section>

<section>
    <h2>Cross-Validation</h2>
    <p>More robust performance estimate by using all data for both training and validation:</p>
    <pre><code class="language-python">from sklearn.model_selection import cross_val_score, StratifiedKFold

# Simple cross-validation
scores = cross_val_score(model, X, y, cv=5)
print(f"Accuracy: {scores.mean():.3f} ± {scores.std():.3f}")

# Explicit stratified k-fold
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
scores = cross_val_score(model, X, y, cv=cv)</code></pre>
    <aside class="notes">5-fold CV trains 5 models, each using 80% data, validates on remaining 20%.</aside>
</section>

<section>
    <h2>Cross-Validation Visualised</h2>
    <pre><code class="language-text">Fold 1: [TEST] [TRAIN] [TRAIN] [TRAIN] [TRAIN] → Score₁
Fold 2: [TRAIN] [TEST] [TRAIN] [TRAIN] [TRAIN] → Score₂
Fold 3: [TRAIN] [TRAIN] [TEST] [TRAIN] [TRAIN] → Score₃
Fold 4: [TRAIN] [TRAIN] [TRAIN] [TEST] [TRAIN] → Score₄
Fold 5: [TRAIN] [TRAIN] [TRAIN] [TRAIN] [TEST] → Score₅

Final Score = Mean(Score₁, Score₂, Score₃, Score₄, Score₅)</code></pre>
    <div class="highlight-box">
        <p>Every sample is used exactly once for validation, providing a more reliable estimate than a single split.</p>
    </div>
    <aside class="notes">CV reduces variance in performance estimates compared to single train/test split.</aside>
</section>

<section>
    <h2>Nested Cross-Validation</h2>
    <p>For unbiased performance estimation when tuning hyperparameters:</p>
    <pre><code class="language-python">from sklearn.model_selection import GridSearchCV, cross_val_score

# Inner loop: hyperparameter selection
inner_cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)
grid_search = GridSearchCV(model, param_grid, cv=inner_cv)

# Outer loop: unbiased performance estimate
outer_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
nested_scores = cross_val_score(grid_search, X, y, cv=outer_cv)

print(f"Unbiased estimate: {nested_scores.mean():.3f}")</code></pre>
    <aside class="notes">Standard CV with GridSearch gives optimistic estimates. Nested CV fixes this.</aside>
</section>

<section>
    <h2>Data Leakage</h2>
    <div class="warning-box">
        <h4>Definition</h4>
        <p>Data leakage occurs when information from outside the training set is used to create the model, leading to overly optimistic performance estimates.</p>
    </div>
    <h4>Common Sources:</h4>
    <ul>
        <li>Preprocessing on full data before splitting</li>
        <li>Feature selection using test labels</li>
        <li>Time-series data split incorrectly</li>
        <li>Duplicate samples across splits</li>
    </ul>
    <aside class="notes">Leakage gives false confidence. Models fail badly in production.</aside>
</section>

<section>
    <h2>Preventing Data Leakage</h2>
    <div class="two-column">
        <div>
            <h4>❌ WRONG</h4>
            <pre><code class="language-python"># Fit scaler on ALL data
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# THEN split
X_train, X_test = ...

# Leakage: scaler learned from test!</code></pre>
        </div>
        <div>
            <h4>✓ CORRECT</h4>
            <pre><code class="language-python"># Split FIRST
X_train, X_test = ...

# Fit scaler on train ONLY
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# OR use Pipeline (recommended)</code></pre>
        </div>
    </div>
    <aside class="notes">Pipelines automatically handle this correctly.</aside>
</section>

<!-- SECTION 5: Metrics (Slides 27-33) -->
<section class="center">
    <h2>Evaluation Metrics</h2>
    <aside class="notes">Choosing appropriate metrics is essential for meaningful evaluation.</aside>
</section>

<section>
    <h2>Confusion Matrix</h2>
    <table>
        <tr>
            <th></th>
            <th>Predicted Negative</th>
            <th>Predicted Positive</th>
        </tr>
        <tr>
            <th>Actual Negative</th>
            <td style="background: rgba(76, 175, 80, 0.3);">True Negative (TN)</td>
            <td style="background: rgba(244, 67, 54, 0.3);">False Positive (FP)</td>
        </tr>
        <tr>
            <th>Actual Positive</th>
            <td style="background: rgba(244, 67, 54, 0.3);">False Negative (FN)</td>
            <td style="background: rgba(76, 175, 80, 0.3);">True Positive (TP)</td>
        </tr>
    </table>
    <pre><code class="language-python">from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y_true, y_pred)
# [[TN, FP],
#  [FN, TP]]</code></pre>
    <aside class="notes">The confusion matrix is the foundation for all classification metrics.</aside>
</section>

<section>
    <h2>Classification Metrics</h2>
    <div class="two-column">
        <div>
            <p><strong>Accuracy</strong> = (TP + TN) / Total</p>
            <p class="small">Overall correctness. Misleading for imbalanced data.</p>
            
            <p><strong>Precision</strong> = TP / (TP + FP)</p>
            <p class="small">"Of predicted positives, how many correct?"</p>
            
            <p><strong>Recall</strong> = TP / (TP + FN)</p>
            <p class="small">"Of actual positives, how many found?"</p>
        </div>
        <div>
            <p><strong>F1 Score</strong> = 2 × (P × R) / (P + R)</p>
            <p class="small">Harmonic mean of precision and recall.</p>
            
            <p><strong>ROC-AUC</strong></p>
            <p class="small">Area under ROC curve. Threshold-independent.</p>
        </div>
    </div>
    <aside class="notes">Choose metrics based on the cost of different error types.</aside>
</section>

<section>
    <h2>When to Use Which Metric?</h2>
    <table>
        <tr>
            <th>Scenario</th>
            <th>Primary Metric</th>
            <th>Reason</th>
        </tr>
        <tr>
            <td>Balanced classes</td>
            <td>Accuracy</td>
            <td>Simple, interpretable</td>
        </tr>
        <tr>
            <td>Imbalanced classes</td>
            <td>F1, ROC-AUC</td>
            <td>Accuracy misleading</td>
        </tr>
        <tr>
            <td>Cost of FP high</td>
            <td>Precision</td>
            <td>Minimise false alarms</td>
        </tr>
        <tr>
            <td>Cost of FN high</td>
            <td>Recall</td>
            <td>Don't miss positives</td>
        </tr>
        <tr>
            <td>Ranking matters</td>
            <td>ROC-AUC</td>
            <td>Threshold-independent</td>
        </tr>
    </table>
    <aside class="notes">Medical diagnosis often prioritises recall; spam detection may prioritise precision.</aside>
</section>

<section>
    <h2>Regression Metrics</h2>
    <div class="two-column">
        <div>
            <p><strong>MSE</strong> = Σ(y - ŷ)² / n</p>
            <p class="small">Penalises large errors heavily.</p>
            
            <p><strong>RMSE</strong> = √MSE</p>
            <p class="small">Same units as target variable.</p>
        </div>
        <div>
            <p><strong>MAE</strong> = Σ|y - ŷ| / n</p>
            <p class="small">Robust to outliers.</p>
            
            <p><strong>R²</strong> = 1 - SS_res / SS_tot</p>
            <p class="small">Proportion of variance explained.</p>
        </div>
    </div>
    <pre><code class="language-python">from sklearn.metrics import mean_squared_error, r2_score
mse = mean_squared_error(y_true, y_pred)
r2 = r2_score(y_true, y_pred)</code></pre>
    <aside class="notes">R² of 1.0 is perfect; 0.0 means predicting the mean is equally good.</aside>
</section>

<section>
    <h2>Clustering Metrics</h2>
    <p>Without ground truth labels:</p>
    <div class="two-column">
        <div>
            <p><strong>Silhouette Score</strong></p>
            <p class="small">Range: [-1, 1]. Higher = better defined clusters.</p>
            
            <p><strong>Calinski-Harabasz Index</strong></p>
            <p class="small">Ratio of between-cluster to within-cluster dispersion.</p>
        </div>
        <div>
            <p><strong>Davies-Bouldin Index</strong></p>
            <p class="small">Lower = better. Average similarity between clusters.</p>
            
            <p><strong>Inertia</strong></p>
            <p class="small">Within-cluster sum of squares. Lower = tighter clusters.</p>
        </div>
    </div>
    <aside class="notes">Clustering evaluation is inherently more subjective than classification.</aside>
</section>

<!-- SECTION 6: Pitfalls (Slides 34-39) -->
<section class="center">
    <h2>Common Pitfalls</h2>
    <aside class="notes">Recognising and avoiding common ML pitfalls is essential for reliable models.</aside>
</section>

<section>
    <h2>Overfitting</h2>
    <div class="warning-box">
        <p>Model learns training data too well, including noise, and fails to generalise.</p>
    </div>
    <h4>Symptoms:</h4>
    <ul>
        <li>High training accuracy, low test accuracy</li>
        <li>Large gap between train and validation scores</li>
        <li>Model complexity exceeds data complexity</li>
    </ul>
    <h4>Solutions:</h4>
    <ul>
        <li>Regularisation (L1, L2)</li>
        <li>Reduce model complexity</li>
        <li>More training data</li>
        <li>Cross-validation for model selection</li>
    </ul>
    <aside class="notes">The bias-variance trade-off: reducing variance often increases bias.</aside>
</section>

<section>
    <h2>Underfitting</h2>
    <div class="warning-box">
        <p>Model is too simple to capture underlying patterns in the data.</p>
    </div>
    <h4>Symptoms:</h4>
    <ul>
        <li>Low training accuracy</li>
        <li>Low test accuracy</li>
        <li>Similar train and test scores (both poor)</li>
    </ul>
    <h4>Solutions:</h4>
    <ul>
        <li>Increase model complexity</li>
        <li>Add more features</li>
        <li>Reduce regularisation</li>
        <li>Feature engineering</li>
    </ul>
    <aside class="notes">Underfitting is often easier to diagnose than overfitting.</aside>
</section>

<section>
    <h2>Class Imbalance</h2>
    <p>When one class dominates the dataset:</p>
    <pre><code class="language-python"># Problem: 95% class A, 5% class B
# Model predicting "always A" gets 95% accuracy!

# Solutions:
# 1. Class weights
model = LogisticRegression(class_weight='balanced')

# 2. Resampling (SMOTE, undersampling)
from imblearn.over_sampling import SMOTE
X_res, y_res = SMOTE().fit_resample(X, y)

# 3. Appropriate metrics
from sklearn.metrics import f1_score, recall_score
# Use F1 or recall instead of accuracy</code></pre>
    <aside class="notes">Accuracy is meaningless for highly imbalanced datasets.</aside>
</section>

<section>
    <h2>Bias-Variance Trade-off</h2>
    <pre><code class="language-text">Total Error = Bias² + Variance + Irreducible Error

High Bias (Underfitting):
  - Model too simple
  - Misses relevant relations
  - Consistent but wrong predictions

High Variance (Overfitting):
  - Model too complex
  - Fits noise in training data
  - Inconsistent predictions

Goal: Find the sweet spot that minimises total error</code></pre>
    <aside class="notes">This fundamental trade-off governs all model selection decisions.</aside>
</section>

<!-- SECTION 7: Unsupervised Learning (Slides 40-45) -->
<section class="center">
    <h2>Unsupervised Learning</h2>
    <aside class="notes">Now we turn to learning without labels.</aside>
</section>

<section>
    <h2>k-Means Clustering</h2>
    <pre><code class="language-python">from sklearn.cluster import KMeans

# Fit k-means with 3 clusters
kmeans = KMeans(n_clusters=3, random_state=42, n_init=10)
labels = kmeans.fit_predict(X)

# Cluster centres
centres = kmeans.cluster_centers_

# Evaluate
from sklearn.metrics import silhouette_score
silhouette = silhouette_score(X, labels)
print(f"Silhouette: {silhouette:.3f}")</code></pre>
    <div class="highlight-box">
        <p>k-means assumes spherical clusters of similar size.</p>
    </div>
    <aside class="notes">k must be specified in advance; use elbow method or silhouette to find optimal k.</aside>
</section>

<section>
    <h2>Finding Optimal k</h2>
    <pre><code class="language-python">from sklearn.metrics import silhouette_score

k_range = range(2, 11)
silhouettes = []

for k in k_range:
    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
    labels = kmeans.fit_predict(X)
    silhouettes.append(silhouette_score(X, labels))

optimal_k = k_range[np.argmax(silhouettes)]
print(f"Optimal k: {optimal_k}")</code></pre>
    <aside class="notes">The silhouette method provides a quantitative approach to selecting k.</aside>
</section>

<section>
    <h2>PCA: Dimensionality Reduction</h2>
    <pre><code class="language-python">from sklearn.decomposition import PCA

# Reduce to 2 dimensions
pca = PCA(n_components=2)
X_2d = pca.fit_transform(X)

# Or keep 95% of variance
pca = PCA(n_components=0.95)
X_reduced = pca.fit_transform(X)

# Explained variance
print(f"Variance explained: {pca.explained_variance_ratio_}")
print(f"Total: {sum(pca.explained_variance_ratio_):.2%}")</code></pre>
    <aside class="notes">PCA finds orthogonal directions of maximum variance.</aside>
</section>

<section>
    <h2>t-SNE for Visualisation</h2>
    <pre><code class="language-python">from sklearn.manifold import TSNE

# Reduce to 2D for visualisation
tsne = TSNE(n_components=2, perplexity=30, random_state=42)
X_2d = tsne.fit_transform(X)

# Plot
plt.scatter(X_2d[:, 0], X_2d[:, 1], c=labels, cmap='viridis')
plt.title('t-SNE Visualisation')
plt.show()</code></pre>
    <div class="warning-box">
        <p>t-SNE is for visualisation only. Don't use for downstream ML tasks.</p>
    </div>
    <aside class="notes">t-SNE preserves local structure but distorts global distances.</aside>
</section>

<!-- SECTION 8: Summary (Slides 46-48) -->
<section>
    <h2>Key Takeaways</h2>
    <ol>
        <li><strong>Paradigm selection</strong>: Match learning type to your data and problem</li>
        <li><strong>Pipelines</strong>: Always use pipelines to prevent data leakage</li>
        <li><strong>Validation</strong>: Use stratified CV; nested CV for tuning</li>
        <li><strong>Metrics</strong>: Choose metrics appropriate for your problem</li>
        <li><strong>Pitfalls</strong>: Watch for overfitting, leakage, and class imbalance</li>
        <li><strong>Unsupervised</strong>: Useful for exploration and preprocessing</li>
    </ol>
    <aside class="notes">These six points summarise the essential knowledge from this unit.</aside>
</section>

<section>
    <h2>Next Steps</h2>
    <div class="two-column">
        <div>
            <h4>Labs</h4>
            <ul>
                <li>Lab 13_01: Supervised Learning</li>
                <li>Lab 13_02: Unsupervised Learning</li>
            </ul>
            <h4>Exercises</h4>
            <ul>
                <li>3 Easy, 3 Medium, 3 Hard</li>
                <li>Build complete pipelines</li>
            </ul>
        </div>
        <div>
            <h4>Assessment</h4>
            <ul>
                <li>Quiz: 10 questions</li>
                <li>Lab submissions</li>
                <li>Self-check rubric</li>
            </ul>
            <h4>14UNIT Preview</h4>
            <ul>
                <li>Parallel ML training</li>
                <li>Distributed computing</li>
            </ul>
        </div>
    </div>
    <aside class="notes">Hands-on practice is essential for mastering these concepts.</aside>
</section>

<section class="center">
    <h1>Questions?</h1>
    <p>Machine Learning for Researchers</p>
    <p class="small">Academy of Economic Studies • Bucharest</p>
    <p class="small">UNIT 13 of 14</p>
    <aside class="notes">Open floor for questions before moving to the labs.</aside>
</section>

</div>
</div>

<script src="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/5.0.4/reveal.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/5.0.4/plugin/notes/notes.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/5.0.4/plugin/highlight/highlight.min.js"></script>
<script>
    Reveal.initialize({
        hash: true,
        slideNumber: true,
        plugins: [ RevealNotes, RevealHighlight ],
        transition: 'slide',
        backgroundTransition: 'fade'
    });
</script>
</body>
</html>
